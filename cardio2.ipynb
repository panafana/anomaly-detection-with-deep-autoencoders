{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ipympl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.utils.data import generate_data\n",
    "from pyod.utils.data import evaluate_print\n",
    "from pyod.utils.example import visualize\n",
    "from pyod.models.combination import aom, moa, average, maximization, median, majority_vote \n",
    "import scipy.io\n",
    "#import matplotlib\n",
    "#matplotlib.use('nbagg')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyod.utils.utility import standardizer\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from tensorflow.python.keras.models import Model\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('cardio.mat')\n",
    "#mat = scipy.io.loadmat('satellite.mat')\n",
    "#mat = scipy.io.loadmat('ionosphere.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1831, 21)\n",
      "(1831,)\n"
     ]
    }
   ],
   "source": [
    "Xtemp = mat['X']\n",
    "ytemp = mat['y']\n",
    "X = np.array(Xtemp)\n",
    "y = np.array(ytemp)\n",
    "print(X.shape)\n",
    "y = y.reshape(X.shape[0],)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "0.0961223375204806\n",
      "(1655, 21)\n",
      "(1831, 22)\n",
      "(352, 22)\n",
      "(352, 22)\n",
      "(352, 21)\n",
      "(352,)\n"
     ]
    }
   ],
   "source": [
    "outliers = 0\n",
    "clean_data = []\n",
    "contam_data = []\n",
    "\n",
    "for i in range(y.shape[0]):\n",
    "    if(y[i]==1.0):\n",
    "        outliers+=1;\n",
    "        contam_data.append(X[i])\n",
    "    else:\n",
    "        clean_data.append(X[i])\n",
    "      \n",
    "        \n",
    "print(outliers)\n",
    "contam = outliers/y.shape[0]\n",
    "print(contam)\n",
    "clean_data = np.array(clean_data)\n",
    "contam_data = np.array(contam_data)\n",
    "print(clean_data.shape)\n",
    "# test_data_full = []\n",
    "# for i in range(176):\n",
    "#     test_data_full.append(contam_data[i])\n",
    "# one = np.ones(176)\n",
    "# test_data_full = np.array(test_data_full)\n",
    "\n",
    "# test_data_full= np.append(test_data_full,one,1)\n",
    "\n",
    "# print(test_data_full.shape)\n",
    "\n",
    "Xy = X.copy()\n",
    "Xy.shape\n",
    "Xy = np.array(Xy)\n",
    "Xy = np.insert(Xy, 21, y, axis=1)\n",
    "print(Xy.shape)\n",
    "count = 176\n",
    "count2 = 176\n",
    "test_data =[]\n",
    "for i in range(Xy.shape[0]):\n",
    "    if(Xy[i,21]==1 and count>0):\n",
    "        test_data.append(Xy[i])\n",
    "        count = count-1\n",
    "    elif(Xy[i,21]==0 and count2>0):\n",
    "        test_data.append(Xy[i])\n",
    "        count2 = count2-1\n",
    "test_data = np.array(test_data)        \n",
    "print(test_data.shape)\n",
    "np.random.shuffle(test_data)\n",
    "print(test_data.shape)\n",
    "test_x = test_data[:,:21]\n",
    "test_y = test_data[:,21]\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "test_x = pd.DataFrame(test_x)\n",
    "test_y = pd.DataFrame(test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = pd.DataFrame(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                440       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                20        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 21)                441       \n",
      "=================================================================\n",
      "Total params: 2,266\n",
      "Trainable params: 2,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1489 samples, validate on 166 samples\n",
      "Epoch 1/100\n",
      "1489/1489 [==============================] - 1s 586us/step - loss: 59.0379 - val_loss: 44.4805\n",
      "Epoch 2/100\n",
      "1489/1489 [==============================] - 0s 146us/step - loss: 38.2624 - val_loss: 31.5723\n",
      "Epoch 3/100\n",
      "1489/1489 [==============================] - 0s 149us/step - loss: 27.3023 - val_loss: 22.4821\n",
      "Epoch 4/100\n",
      "1489/1489 [==============================] - 0s 144us/step - loss: 20.2045 - val_loss: 17.0341\n",
      "Epoch 5/100\n",
      "1489/1489 [==============================] - 0s 141us/step - loss: 15.9533 - val_loss: 13.5252\n",
      "Epoch 6/100\n",
      "1489/1489 [==============================] - 0s 139us/step - loss: 13.1615 - val_loss: 11.0785\n",
      "Epoch 7/100\n",
      "1489/1489 [==============================] - 0s 130us/step - loss: 10.9879 - val_loss: 9.3314\n",
      "Epoch 8/100\n",
      "1489/1489 [==============================] - 0s 134us/step - loss: 9.3845 - val_loss: 8.0498\n",
      "Epoch 9/100\n",
      "1489/1489 [==============================] - 0s 137us/step - loss: 8.3377 - val_loss: 7.0760\n",
      "Epoch 10/100\n",
      "1489/1489 [==============================] - 0s 133us/step - loss: 7.5811 - val_loss: 6.3420\n",
      "Epoch 11/100\n",
      "1489/1489 [==============================] - 0s 133us/step - loss: 6.8090 - val_loss: 5.7324\n",
      "Epoch 12/100\n",
      "1489/1489 [==============================] - 0s 142us/step - loss: 6.2821 - val_loss: 5.2660\n",
      "Epoch 13/100\n",
      "1489/1489 [==============================] - 0s 155us/step - loss: 5.8709 - val_loss: 4.8757\n",
      "Epoch 14/100\n",
      "1489/1489 [==============================] - 0s 143us/step - loss: 5.4169 - val_loss: 4.5670\n",
      "Epoch 15/100\n",
      "1489/1489 [==============================] - 0s 143us/step - loss: 5.2070 - val_loss: 4.3003\n",
      "Epoch 16/100\n",
      "1489/1489 [==============================] - 0s 142us/step - loss: 4.8633 - val_loss: 4.0774\n",
      "Epoch 17/100\n",
      "1489/1489 [==============================] - 0s 139us/step - loss: 4.6016 - val_loss: 3.8910\n",
      "Epoch 18/100\n",
      "1489/1489 [==============================] - 0s 143us/step - loss: 4.4104 - val_loss: 3.7218\n",
      "Epoch 19/100\n",
      "1489/1489 [==============================] - 0s 128us/step - loss: 4.2089 - val_loss: 3.5712\n",
      "Epoch 20/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 4.1221 - val_loss: 3.4380\n",
      "Epoch 21/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 3.9060 - val_loss: 3.3211\n",
      "Epoch 22/100\n",
      "1489/1489 [==============================] - 0s 143us/step - loss: 3.7430 - val_loss: 3.2180\n",
      "Epoch 23/100\n",
      "1489/1489 [==============================] - 0s 135us/step - loss: 3.6076 - val_loss: 3.1225\n",
      "Epoch 24/100\n",
      "1489/1489 [==============================] - 0s 128us/step - loss: 3.5031 - val_loss: 3.0334\n",
      "Epoch 25/100\n",
      "1489/1489 [==============================] - 0s 127us/step - loss: 3.3875 - val_loss: 2.9533\n",
      "Epoch 26/100\n",
      "1489/1489 [==============================] - 0s 128us/step - loss: 3.3129 - val_loss: 2.8814\n",
      "Epoch 27/100\n",
      "1489/1489 [==============================] - 0s 128us/step - loss: 3.1878 - val_loss: 2.8136\n",
      "Epoch 28/100\n",
      "1489/1489 [==============================] - 0s 131us/step - loss: 3.0968 - val_loss: 2.7501\n",
      "Epoch 29/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 3.0185 - val_loss: 2.6912\n",
      "Epoch 30/100\n",
      "1489/1489 [==============================] - 0s 128us/step - loss: 2.9369 - val_loss: 2.6357\n",
      "Epoch 31/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 2.8671 - val_loss: 2.5828\n",
      "Epoch 32/100\n",
      "1489/1489 [==============================] - 0s 142us/step - loss: 2.7964 - val_loss: 2.5349\n",
      "Epoch 33/100\n",
      "1489/1489 [==============================] - 0s 136us/step - loss: 2.7411 - val_loss: 2.4900\n",
      "Epoch 34/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 2.6640 - val_loss: 2.4467\n",
      "Epoch 35/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 2.6139 - val_loss: 2.4053\n",
      "Epoch 36/100\n",
      "1489/1489 [==============================] - 0s 127us/step - loss: 2.5514 - val_loss: 2.3670\n",
      "Epoch 37/100\n",
      "1489/1489 [==============================] - 0s 174us/step - loss: 2.4874 - val_loss: 2.3294\n",
      "Epoch 38/100\n",
      "1489/1489 [==============================] - 0s 164us/step - loss: 2.4584 - val_loss: 2.2942\n",
      "Epoch 39/100\n",
      "1489/1489 [==============================] - 0s 140us/step - loss: 2.4076 - val_loss: 2.2609\n",
      "Epoch 40/100\n",
      "1489/1489 [==============================] - 0s 135us/step - loss: 2.3577 - val_loss: 2.2275\n",
      "Epoch 41/100\n",
      "1489/1489 [==============================] - 0s 139us/step - loss: 2.3236 - val_loss: 2.1976\n",
      "Epoch 42/100\n",
      "1489/1489 [==============================] - 0s 139us/step - loss: 2.2733 - val_loss: 2.1685\n",
      "Epoch 43/100\n",
      "1489/1489 [==============================] - 0s 137us/step - loss: 2.2283 - val_loss: 2.1399\n",
      "Epoch 44/100\n",
      "1489/1489 [==============================] - 0s 138us/step - loss: 2.1743 - val_loss: 2.1121\n",
      "Epoch 45/100\n",
      "1489/1489 [==============================] - 0s 133us/step - loss: 2.1784 - val_loss: 2.0873\n",
      "Epoch 46/100\n",
      "1489/1489 [==============================] - 0s 131us/step - loss: 2.1195 - val_loss: 2.0607\n",
      "Epoch 47/100\n",
      "1489/1489 [==============================] - 0s 145us/step - loss: 2.1143 - val_loss: 2.0374\n",
      "Epoch 48/100\n",
      "1489/1489 [==============================] - 0s 147us/step - loss: 2.0796 - val_loss: 2.0144\n",
      "Epoch 49/100\n",
      "1489/1489 [==============================] - 0s 131us/step - loss: 2.0350 - val_loss: 1.9912\n",
      "Epoch 50/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 2.0184 - val_loss: 1.9694\n",
      "Epoch 51/100\n",
      "1489/1489 [==============================] - 0s 137us/step - loss: 1.9848 - val_loss: 1.9479\n",
      "Epoch 52/100\n",
      "1489/1489 [==============================] - 0s 144us/step - loss: 1.9609 - val_loss: 1.9281\n",
      "Epoch 53/100\n",
      "1489/1489 [==============================] - 0s 136us/step - loss: 1.9378 - val_loss: 1.9087\n",
      "Epoch 54/100\n",
      "1489/1489 [==============================] - 0s 134us/step - loss: 1.9175 - val_loss: 1.8890\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489/1489 [==============================] - 0s 129us/step - loss: 1.8946 - val_loss: 1.8706\n",
      "Epoch 56/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 1.8825 - val_loss: 1.8523\n",
      "Epoch 57/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 1.8608 - val_loss: 1.8348\n",
      "Epoch 58/100\n",
      "1489/1489 [==============================] - 0s 133us/step - loss: 1.8374 - val_loss: 1.8178\n",
      "Epoch 59/100\n",
      "1489/1489 [==============================] - 0s 131us/step - loss: 1.8120 - val_loss: 1.8011\n",
      "Epoch 60/100\n",
      "1489/1489 [==============================] - 0s 134us/step - loss: 1.7955 - val_loss: 1.7856\n",
      "Epoch 61/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 1.7776 - val_loss: 1.7692\n",
      "Epoch 62/100\n",
      "1489/1489 [==============================] - 0s 130us/step - loss: 1.7582 - val_loss: 1.7538\n",
      "Epoch 63/100\n",
      "1489/1489 [==============================] - 0s 134us/step - loss: 1.7443 - val_loss: 1.7393\n",
      "Epoch 64/100\n",
      "1489/1489 [==============================] - 0s 136us/step - loss: 1.7226 - val_loss: 1.7248\n",
      "Epoch 65/100\n",
      "1489/1489 [==============================] - 0s 147us/step - loss: 1.7118 - val_loss: 1.7111\n",
      "Epoch 66/100\n",
      "1489/1489 [==============================] - 0s 138us/step - loss: 1.7058 - val_loss: 1.6977\n",
      "Epoch 67/100\n",
      "1489/1489 [==============================] - 0s 157us/step - loss: 1.6856 - val_loss: 1.6837\n",
      "Epoch 68/100\n",
      "1489/1489 [==============================] - 0s 155us/step - loss: 1.6666 - val_loss: 1.6703\n",
      "Epoch 69/100\n",
      "1489/1489 [==============================] - 0s 149us/step - loss: 1.6558 - val_loss: 1.6579\n",
      "Epoch 70/100\n",
      "1489/1489 [==============================] - 0s 135us/step - loss: 1.6372 - val_loss: 1.6455\n",
      "Epoch 71/100\n",
      "1489/1489 [==============================] - 0s 127us/step - loss: 1.6266 - val_loss: 1.6329\n",
      "Epoch 72/100\n",
      "1489/1489 [==============================] - 0s 141us/step - loss: 1.6153 - val_loss: 1.6216\n",
      "Epoch 73/100\n",
      "1489/1489 [==============================] - 0s 135us/step - loss: 1.6010 - val_loss: 1.6103\n",
      "Epoch 74/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 1.5915 - val_loss: 1.5985\n",
      "Epoch 75/100\n",
      "1489/1489 [==============================] - 0s 128us/step - loss: 1.5805 - val_loss: 1.5879\n",
      "Epoch 76/100\n",
      "1489/1489 [==============================] - 0s 134us/step - loss: 1.5666 - val_loss: 1.5765\n",
      "Epoch 77/100\n",
      "1489/1489 [==============================] - 0s 135us/step - loss: 1.5551 - val_loss: 1.5663\n",
      "Epoch 78/100\n",
      "1489/1489 [==============================] - 0s 139us/step - loss: 1.5452 - val_loss: 1.5563\n",
      "Epoch 79/100\n",
      "1489/1489 [==============================] - 0s 137us/step - loss: 1.5270 - val_loss: 1.5455\n",
      "Epoch 80/100\n",
      "1489/1489 [==============================] - 0s 132us/step - loss: 1.5258 - val_loss: 1.5361\n",
      "Epoch 81/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 1.5076 - val_loss: 1.5262\n",
      "Epoch 82/100\n",
      "1489/1489 [==============================] - 0s 144us/step - loss: 1.5029 - val_loss: 1.5165\n",
      "Epoch 83/100\n",
      "1489/1489 [==============================] - 0s 204us/step - loss: 1.4899 - val_loss: 1.5075\n",
      "Epoch 84/100\n",
      "1489/1489 [==============================] - 0s 159us/step - loss: 1.4806 - val_loss: 1.4985\n",
      "Epoch 85/100\n",
      "1489/1489 [==============================] - 0s 132us/step - loss: 1.4688 - val_loss: 1.4891\n",
      "Epoch 86/100\n",
      "1489/1489 [==============================] - 0s 132us/step - loss: 1.4632 - val_loss: 1.4810\n",
      "Epoch 87/100\n",
      "1489/1489 [==============================] - 0s 135us/step - loss: 1.4524 - val_loss: 1.4723\n",
      "Epoch 88/100\n",
      "1489/1489 [==============================] - 0s 140us/step - loss: 1.4437 - val_loss: 1.4639\n",
      "Epoch 89/100\n",
      "1489/1489 [==============================] - 0s 147us/step - loss: 1.4367 - val_loss: 1.4556\n",
      "Epoch 90/100\n",
      "1489/1489 [==============================] - 0s 137us/step - loss: 1.4234 - val_loss: 1.4478\n",
      "Epoch 91/100\n",
      "1489/1489 [==============================] - 0s 144us/step - loss: 1.4177 - val_loss: 1.4401\n",
      "Epoch 92/100\n",
      "1489/1489 [==============================] - 0s 135us/step - loss: 1.4118 - val_loss: 1.4326\n",
      "Epoch 93/100\n",
      "1489/1489 [==============================] - 0s 130us/step - loss: 1.4019 - val_loss: 1.4251\n",
      "Epoch 94/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 1.3931 - val_loss: 1.4174\n",
      "Epoch 95/100\n",
      "1489/1489 [==============================] - 0s 136us/step - loss: 1.3838 - val_loss: 1.4099\n",
      "Epoch 96/100\n",
      "1489/1489 [==============================] - 0s 137us/step - loss: 1.3766 - val_loss: 1.4032\n",
      "Epoch 97/100\n",
      "1489/1489 [==============================] - 0s 130us/step - loss: 1.3716 - val_loss: 1.3960\n",
      "Epoch 98/100\n",
      "1489/1489 [==============================] - 0s 132us/step - loss: 1.3632 - val_loss: 1.3891\n",
      "Epoch 99/100\n",
      "1489/1489 [==============================] - 0s 135us/step - loss: 1.3519 - val_loss: 1.3825\n",
      "Epoch 100/100\n",
      "1489/1489 [==============================] - 0s 131us/step - loss: 1.3493 - val_loss: 1.3759\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoEncoder(batch_size=32, contamination=0.1, dropout_rate=0.2, epochs=100,\n",
       "      hidden_activation='relu', hidden_neurons=[20, 10, 1, 10, 20],\n",
       "      l2_regularizer=0.1,\n",
       "      loss=<function mean_squared_error at 0x0000013614087288>,\n",
       "      optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "      random_state=None, validation_size=0.1, verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1 = AutoEncoder(hidden_neurons =[ 20, 10, 1,  10, 20]\n",
    "                   #,contamination = contam\n",
    "                   ,epochs=epochs\n",
    "                  )\n",
    "\n",
    "clf1.fit(clean_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Get the outlier scores for the train data\n",
    "y_train_scores = clf1.decision_scores_  \n",
    "\n",
    "# Predict the anomaly scores\n",
    "y_train_pred = clf1.labels_\n",
    "y_test_scores = clf1.decision_function(test_x)  # outlier scores\n",
    "y_train_scores = clf1.decision_scores_  # raw outlier scores\n",
    "#print(y_test_scores.shape)\n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "y_test_pred = clf1.predict(test_x)\n",
    " \n",
    "# Plot it!\n",
    "plt.close()\n",
    "plt.hist(y_test_scores, bins='auto',density=True)  \n",
    "plt.title(\"Histogram for Model Clf1 Anomaly Scores\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On Training Data:\n",
      "\n",
      "On Test Data:\n",
      "clf_1 ROC:0.9281, precision @ rank n:0.8629\n"
     ]
    }
   ],
   "source": [
    "# evaluate and print the results\n",
    "print(\"\\nOn Training Data:\")\n",
    "#evaluate_print('clf_1', y_train, y_train_scores)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print('clf_1', test_y, y_test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    183\n",
      "0    169\n",
      "Name: cluster, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051246</td>\n",
       "      <td>-0.115602</td>\n",
       "      <td>0.011678</td>\n",
       "      <td>-0.549257</td>\n",
       "      <td>-0.410944</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>0.024669</td>\n",
       "      <td>-0.287349</td>\n",
       "      <td>0.008566</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070244</td>\n",
       "      <td>0.025036</td>\n",
       "      <td>0.048608</td>\n",
       "      <td>-0.118998</td>\n",
       "      <td>0.083768</td>\n",
       "      <td>0.221448</td>\n",
       "      <td>0.102950</td>\n",
       "      <td>-0.448091</td>\n",
       "      <td>0.125238</td>\n",
       "      <td>4.293477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.090496</td>\n",
       "      <td>-0.689559</td>\n",
       "      <td>0.677958</td>\n",
       "      <td>-0.198802</td>\n",
       "      <td>0.674566</td>\n",
       "      <td>0.481160</td>\n",
       "      <td>1.793747</td>\n",
       "      <td>0.792592</td>\n",
       "      <td>0.651487</td>\n",
       "      <td>0.838603</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.479786</td>\n",
       "      <td>0.156597</td>\n",
       "      <td>0.363216</td>\n",
       "      <td>0.169989</td>\n",
       "      <td>-1.254971</td>\n",
       "      <td>-1.286906</td>\n",
       "      <td>-1.164053</td>\n",
       "      <td>1.158110</td>\n",
       "      <td>-0.564696</td>\n",
       "      <td>11.739944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "cluster                                                                         \n",
       "0        0.051246 -0.115602  0.011678 -0.549257 -0.410944 -0.061401 -0.278295   \n",
       "1       -0.090496 -0.689559  0.677958 -0.198802  0.674566  0.481160  1.793747   \n",
       "\n",
       "                7         8         9  ...        12        13        14  \\\n",
       "cluster                                ...                                 \n",
       "0        0.024669 -0.287349  0.008566  ... -0.070244  0.025036  0.048608   \n",
       "1        0.792592  0.651487  0.838603  ... -0.479786  0.156597  0.363216   \n",
       "\n",
       "               15        16        17        18        19        20      score  \n",
       "cluster                                                                         \n",
       "0       -0.118998  0.083768  0.221448  0.102950 -0.448091  0.125238   4.293477  \n",
       "1        0.169989 -1.254971 -1.286906 -1.164053  1.158110 -0.564696  11.739944  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = test_x.copy()\n",
    "df_test['score'] = y_test_scores\n",
    "df_test['cluster'] = np.where(df_test['score']<6.3, 0, 1)\n",
    "print(df_test['cluster'].value_counts())\n",
    "\n",
    "df_test.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Data Counts\n",
      "{0: 164, 1: 188}\n"
     ]
    }
   ],
   "source": [
    "#print(\"Test Data Counts\")\n",
    "#unique, counts = np.unique(y_test, return_counts=True)\n",
    "#print(dict(zip(unique, counts)))\n",
    "print(\"Predicted Data Counts\")\n",
    "unique, counts = np.unique(y_test_pred, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                440       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 12)                252       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 78        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 14        \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 6)                 18        \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 12)                84        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 20)                260       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 21)                441       \n",
      "=================================================================\n",
      "Total params: 2,511\n",
      "Trainable params: 2,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1489 samples, validate on 166 samples\n",
      "Epoch 1/100\n",
      "1489/1489 [==============================] - 1s 544us/step - loss: 66.7689 - val_loss: 47.8754\n",
      "Epoch 2/100\n",
      "1489/1489 [==============================] - 0s 177us/step - loss: 40.0812 - val_loss: 33.0314\n",
      "Epoch 3/100\n",
      "1489/1489 [==============================] - 0s 173us/step - loss: 27.8115 - val_loss: 23.9167\n",
      "Epoch 4/100\n",
      "1489/1489 [==============================] - 0s 174us/step - loss: 20.6360 - val_loss: 18.3763\n",
      "Epoch 5/100\n",
      "1489/1489 [==============================] - 0s 184us/step - loss: 16.1162 - val_loss: 14.6267\n",
      "Epoch 6/100\n",
      "1489/1489 [==============================] - 0s 182us/step - loss: 13.3354 - val_loss: 12.0369\n",
      "Epoch 7/100\n",
      "1489/1489 [==============================] - 0s 182us/step - loss: 10.9924 - val_loss: 10.1765\n",
      "Epoch 8/100\n",
      "1489/1489 [==============================] - 0s 172us/step - loss: 9.4422 - val_loss: 8.7759\n",
      "Epoch 9/100\n",
      "1489/1489 [==============================] - 0s 171us/step - loss: 8.4122 - val_loss: 7.6862\n",
      "Epoch 10/100\n",
      "1489/1489 [==============================] - 0s 170us/step - loss: 7.3648 - val_loss: 6.8510\n",
      "Epoch 11/100\n",
      "1489/1489 [==============================] - 0s 171us/step - loss: 6.7402 - val_loss: 6.1954\n",
      "Epoch 12/100\n",
      "1489/1489 [==============================] - 0s 171us/step - loss: 6.2650 - val_loss: 5.6779\n",
      "Epoch 13/100\n",
      "1489/1489 [==============================] - 0s 171us/step - loss: 5.7256 - val_loss: 5.2310\n",
      "Epoch 14/100\n",
      "1489/1489 [==============================] - 0s 172us/step - loss: 5.4475 - val_loss: 4.8564\n",
      "Epoch 15/100\n",
      "1489/1489 [==============================] - 0s 169us/step - loss: 5.0784 - val_loss: 4.5481\n",
      "Epoch 16/100\n",
      "1489/1489 [==============================] - 0s 180us/step - loss: 4.8495 - val_loss: 4.2865\n",
      "Epoch 17/100\n",
      "1489/1489 [==============================] - 0s 176us/step - loss: 4.6744 - val_loss: 4.0628\n",
      "Epoch 18/100\n",
      "1489/1489 [==============================] - 0s 165us/step - loss: 4.3591 - val_loss: 3.8569\n",
      "Epoch 19/100\n",
      "1489/1489 [==============================] - 0s 184us/step - loss: 4.1590 - val_loss: 3.6820\n",
      "Epoch 20/100\n",
      "1489/1489 [==============================] - 0s 180us/step - loss: 3.9901 - val_loss: 3.5348\n",
      "Epoch 21/100\n",
      "1489/1489 [==============================] - 0s 162us/step - loss: 3.8771 - val_loss: 3.3920\n",
      "Epoch 22/100\n",
      "1489/1489 [==============================] - 0s 180us/step - loss: 3.6931 - val_loss: 3.2784\n",
      "Epoch 23/100\n",
      "1489/1489 [==============================] - 0s 184us/step - loss: 3.3732 - val_loss: 3.1686\n",
      "Epoch 24/100\n",
      "1489/1489 [==============================] - 0s 165us/step - loss: 3.4795 - val_loss: 3.0707\n",
      "Epoch 25/100\n",
      "1489/1489 [==============================] - 0s 186us/step - loss: 3.3675 - val_loss: 2.9794\n",
      "Epoch 26/100\n",
      "1489/1489 [==============================] - 0s 152us/step - loss: 3.2505 - val_loss: 2.9007\n",
      "Epoch 27/100\n",
      "1489/1489 [==============================] - 0s 159us/step - loss: 3.2014 - val_loss: 2.8266\n",
      "Epoch 28/100\n",
      "1489/1489 [==============================] - 0s 155us/step - loss: 3.0838 - val_loss: 2.7613\n",
      "Epoch 29/100\n",
      "1489/1489 [==============================] - 0s 154us/step - loss: 2.9901 - val_loss: 2.6964\n",
      "Epoch 30/100\n",
      "1489/1489 [==============================] - 0s 149us/step - loss: 2.9205 - val_loss: 2.6397\n",
      "Epoch 31/100\n",
      "1489/1489 [==============================] - 0s 161us/step - loss: 2.8841 - val_loss: 2.5862\n",
      "Epoch 32/100\n",
      "1489/1489 [==============================] - 0s 196us/step - loss: 2.7967 - val_loss: 2.5371\n",
      "Epoch 33/100\n",
      "1489/1489 [==============================] - 0s 151us/step - loss: 2.7138 - val_loss: 2.4900\n",
      "Epoch 34/100\n",
      "1489/1489 [==============================] - 0s 145us/step - loss: 2.6487 - val_loss: 2.4463\n",
      "Epoch 35/100\n",
      "1489/1489 [==============================] - 0s 154us/step - loss: 2.5912 - val_loss: 2.4059\n",
      "Epoch 36/100\n",
      "1489/1489 [==============================] - 0s 149us/step - loss: 2.5538 - val_loss: 2.3681\n",
      "Epoch 37/100\n",
      "1489/1489 [==============================] - 0s 172us/step - loss: 2.4782 - val_loss: 2.3304\n",
      "Epoch 38/100\n",
      "1489/1489 [==============================] - 0s 180us/step - loss: 2.4520 - val_loss: 2.2960\n",
      "Epoch 39/100\n",
      "1489/1489 [==============================] - 0s 192us/step - loss: 2.3931 - val_loss: 2.2622\n",
      "Epoch 40/100\n",
      "1489/1489 [==============================] - 0s 151us/step - loss: 2.3552 - val_loss: 2.2318\n",
      "Epoch 41/100\n",
      "1489/1489 [==============================] - 0s 179us/step - loss: 2.2981 - val_loss: 2.2022\n",
      "Epoch 42/100\n",
      "1489/1489 [==============================] - 0s 186us/step - loss: 2.2650 - val_loss: 2.1732\n",
      "Epoch 43/100\n",
      "1489/1489 [==============================] - 0s 160us/step - loss: 2.2236 - val_loss: 2.1466\n",
      "Epoch 44/100\n",
      "1489/1489 [==============================] - 0s 207us/step - loss: 2.1903 - val_loss: 2.1210\n",
      "Epoch 45/100\n",
      "1489/1489 [==============================] - 0s 159us/step - loss: 2.1593 - val_loss: 2.0955\n",
      "Epoch 46/100\n",
      "1489/1489 [==============================] - 0s 147us/step - loss: 2.1158 - val_loss: 2.0720\n",
      "Epoch 47/100\n",
      "1489/1489 [==============================] - 0s 154us/step - loss: 2.0918 - val_loss: 2.0488\n",
      "Epoch 48/100\n",
      "1489/1489 [==============================] - 0s 153us/step - loss: 2.0584 - val_loss: 2.0264\n",
      "Epoch 49/100\n",
      "1489/1489 [==============================] - 0s 153us/step - loss: 2.0257 - val_loss: 2.0049\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489/1489 [==============================] - 0s 153us/step - loss: 2.0095 - val_loss: 1.9847\n",
      "Epoch 51/100\n",
      "1489/1489 [==============================] - 0s 154us/step - loss: 1.9790 - val_loss: 1.9655\n",
      "Epoch 52/100\n",
      "1489/1489 [==============================] - 0s 159us/step - loss: 1.9593 - val_loss: 1.9460\n",
      "Epoch 53/100\n",
      "1489/1489 [==============================] - 0s 154us/step - loss: 1.9243 - val_loss: 1.9268\n",
      "Epoch 54/100\n",
      "1489/1489 [==============================] - 0s 149us/step - loss: 1.9205 - val_loss: 1.9101\n",
      "Epoch 55/100\n",
      "1489/1489 [==============================] - 0s 150us/step - loss: 1.8817 - val_loss: 1.8912\n",
      "Epoch 56/100\n",
      "1489/1489 [==============================] - 0s 155us/step - loss: 1.8527 - val_loss: 1.8744\n",
      "Epoch 57/100\n",
      "1489/1489 [==============================] - 0s 155us/step - loss: 1.8447 - val_loss: 1.8582\n",
      "Epoch 58/100\n",
      "1489/1489 [==============================] - 0s 152us/step - loss: 1.8167 - val_loss: 1.8423\n",
      "Epoch 59/100\n",
      "1489/1489 [==============================] - 0s 156us/step - loss: 1.7934 - val_loss: 1.8261\n",
      "Epoch 60/100\n",
      "1489/1489 [==============================] - 0s 155us/step - loss: 1.7732 - val_loss: 1.8113\n",
      "Epoch 61/100\n",
      "1489/1489 [==============================] - 0s 154us/step - loss: 1.7643 - val_loss: 1.7974\n",
      "Epoch 62/100\n",
      "1489/1489 [==============================] - 0s 151us/step - loss: 1.7431 - val_loss: 1.7825\n",
      "Epoch 63/100\n",
      "1489/1489 [==============================] - 0s 157us/step - loss: 1.7253 - val_loss: 1.7685\n",
      "Epoch 64/100\n",
      "1489/1489 [==============================] - 0s 149us/step - loss: 1.7067 - val_loss: 1.7554\n",
      "Epoch 65/100\n",
      "1489/1489 [==============================] - 0s 154us/step - loss: 1.6857 - val_loss: 1.7422\n",
      "Epoch 66/100\n",
      "1489/1489 [==============================] - 0s 149us/step - loss: 1.6753 - val_loss: 1.7295\n",
      "Epoch 67/100\n",
      "1489/1489 [==============================] - 0s 150us/step - loss: 1.6661 - val_loss: 1.7169\n",
      "Epoch 68/100\n",
      "1489/1489 [==============================] - 0s 150us/step - loss: 1.6398 - val_loss: 1.7044\n",
      "Epoch 69/100\n",
      "1489/1489 [==============================] - 0s 157us/step - loss: 1.6275 - val_loss: 1.6924\n",
      "Epoch 70/100\n",
      "1489/1489 [==============================] - 0s 154us/step - loss: 1.6221 - val_loss: 1.6815\n",
      "Epoch 71/100\n",
      "1489/1489 [==============================] - 0s 151us/step - loss: 1.6004 - val_loss: 1.6695\n",
      "Epoch 72/100\n",
      "1489/1489 [==============================] - 0s 151us/step - loss: 1.5961 - val_loss: 1.6593\n",
      "Epoch 73/100\n",
      "1489/1489 [==============================] - 0s 149us/step - loss: 1.5792 - val_loss: 1.6474\n",
      "Epoch 74/100\n",
      "1489/1489 [==============================] - 0s 154us/step - loss: 1.5750 - val_loss: 1.6379\n",
      "Epoch 75/100\n",
      "1489/1489 [==============================] - 0s 157us/step - loss: 1.5535 - val_loss: 1.6261\n",
      "Epoch 76/100\n",
      "1489/1489 [==============================] - 0s 163us/step - loss: 1.5476 - val_loss: 1.6163\n",
      "Epoch 77/100\n",
      "1489/1489 [==============================] - 0s 151us/step - loss: 1.5383 - val_loss: 1.6068\n",
      "Epoch 78/100\n",
      "1489/1489 [==============================] - 0s 152us/step - loss: 1.5296 - val_loss: 1.5975\n",
      "Epoch 79/100\n",
      "1489/1489 [==============================] - 0s 156us/step - loss: 1.5126 - val_loss: 1.5874\n",
      "Epoch 80/100\n",
      "1489/1489 [==============================] - 0s 155us/step - loss: 1.4974 - val_loss: 1.5773\n",
      "Epoch 81/100\n",
      "1489/1489 [==============================] - 0s 184us/step - loss: 1.4911 - val_loss: 1.5686\n",
      "Epoch 82/100\n",
      "1489/1489 [==============================] - 0s 179us/step - loss: 1.4828 - val_loss: 1.5596\n",
      "Epoch 83/100\n",
      "1489/1489 [==============================] - 0s 196us/step - loss: 1.4689 - val_loss: 1.5506\n",
      "Epoch 84/100\n",
      "1489/1489 [==============================] - 0s 159us/step - loss: 1.4646 - val_loss: 1.5421\n",
      "Epoch 85/100\n",
      "1489/1489 [==============================] - 0s 168us/step - loss: 1.4563 - val_loss: 1.5337\n",
      "Epoch 86/100\n",
      "1489/1489 [==============================] - 0s 174us/step - loss: 1.4454 - val_loss: 1.5255\n",
      "Epoch 87/100\n",
      "1489/1489 [==============================] - 0s 175us/step - loss: 1.4381 - val_loss: 1.5174\n",
      "Epoch 88/100\n",
      "1489/1489 [==============================] - 0s 174us/step - loss: 1.4282 - val_loss: 1.5091\n",
      "Epoch 89/100\n",
      "1489/1489 [==============================] - 0s 175us/step - loss: 1.4205 - val_loss: 1.5017\n",
      "Epoch 90/100\n",
      "1489/1489 [==============================] - 0s 160us/step - loss: 1.4064 - val_loss: 1.4937\n",
      "Epoch 91/100\n",
      "1489/1489 [==============================] - 0s 150us/step - loss: 1.3987 - val_loss: 1.4866\n",
      "Epoch 92/100\n",
      "1489/1489 [==============================] - 0s 150us/step - loss: 1.3932 - val_loss: 1.4790\n",
      "Epoch 93/100\n",
      "1489/1489 [==============================] - 0s 157us/step - loss: 1.3881 - val_loss: 1.4724\n",
      "Epoch 94/100\n",
      "1489/1489 [==============================] - 0s 172us/step - loss: 1.3805 - val_loss: 1.4654\n",
      "Epoch 95/100\n",
      "1489/1489 [==============================] - 0s 166us/step - loss: 1.3686 - val_loss: 1.4580\n",
      "Epoch 96/100\n",
      "1489/1489 [==============================] - 0s 164us/step - loss: 1.3682 - val_loss: 1.4515\n",
      "Epoch 97/100\n",
      "1489/1489 [==============================] - 0s 158us/step - loss: 1.3562 - val_loss: 1.4446\n",
      "Epoch 98/100\n",
      "1489/1489 [==============================] - 0s 166us/step - loss: 1.3450 - val_loss: 1.4380\n",
      "Epoch 99/100\n",
      "1489/1489 [==============================] - 0s 178us/step - loss: 1.3399 - val_loss: 1.4314\n",
      "Epoch 100/100\n",
      "1489/1489 [==============================] - 0s 170us/step - loss: 1.3322 - val_loss: 1.4254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoEncoder(batch_size=32, contamination=0.1, dropout_rate=0.2, epochs=100,\n",
       "      hidden_activation='relu', hidden_neurons=[20, 12, 6, 2, 6, 12, 20],\n",
       "      l2_regularizer=0.1,\n",
       "      loss=<function mean_squared_error at 0x0000013614087288>,\n",
       "      optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "      random_state=None, validation_size=0.1, verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2 = AutoEncoder(hidden_neurons =[20, 12, 6,  \n",
    "                                    2, \n",
    "                                    6,  12, 20]\n",
    "                #   ,contamination = contam\n",
    "                   ,epochs=epochs\n",
    "                  )\n",
    "clf2.fit(clean_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Predict the anomaly scores\n",
    "y_train_pred2 = clf2.labels_\n",
    "y_test_scores2 = clf2.decision_function(test_x)  \n",
    "\n",
    "y_test_scores2 = pd.Series(y_test_scores2)\n",
    "y_train_scores2 = clf2.decision_scores_  # raw outlier scores\n",
    "y_test_pred2 = clf2.predict(test_x)\n",
    "\n",
    "\n",
    "\n",
    "plt.hist(y_test_scores2, bins='auto')  \n",
    "plt.title(\"Histogram for Model Clf2 Anomaly Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On Training Data:\n",
      "\n",
      "On Test Data:\n",
      "clf_2 ROC:0.9281, precision @ rank n:0.8629\n"
     ]
    }
   ],
   "source": [
    "# evaluate and print the results\n",
    "print(\"\\nOn Training Data:\")\n",
    "#evaluate_print('clf_2', y_train2, y_train_scores2)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print('clf_2', test_y, y_test_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    186\n",
      "0    166\n",
      "Name: cluster, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038060</td>\n",
       "      <td>-0.101065</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>-0.541590</td>\n",
       "      <td>-0.422266</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>-0.294400</td>\n",
       "      <td>-0.003552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085423</td>\n",
       "      <td>0.016819</td>\n",
       "      <td>0.054813</td>\n",
       "      <td>-0.121067</td>\n",
       "      <td>0.078747</td>\n",
       "      <td>0.217730</td>\n",
       "      <td>0.098549</td>\n",
       "      <td>-0.448940</td>\n",
       "      <td>0.136416</td>\n",
       "      <td>4.257245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.076442</td>\n",
       "      <td>-0.693276</td>\n",
       "      <td>0.669675</td>\n",
       "      <td>-0.211297</td>\n",
       "      <td>0.667162</td>\n",
       "      <td>0.472409</td>\n",
       "      <td>1.760327</td>\n",
       "      <td>0.788247</td>\n",
       "      <td>0.642638</td>\n",
       "      <td>0.836030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459634</td>\n",
       "      <td>0.161808</td>\n",
       "      <td>0.352604</td>\n",
       "      <td>0.167175</td>\n",
       "      <td>-1.228897</td>\n",
       "      <td>-1.259259</td>\n",
       "      <td>-1.139690</td>\n",
       "      <td>1.132961</td>\n",
       "      <td>-0.563544</td>\n",
       "      <td>11.653811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "cluster                                                                         \n",
       "0        0.038060 -0.101065  0.008917 -0.541590 -0.422266 -0.061401 -0.278295   \n",
       "1       -0.076442 -0.693276  0.669675 -0.211297  0.667162  0.472409  1.760327   \n",
       "\n",
       "                7         8         9  ...        12        13        14  \\\n",
       "cluster                                ...                                 \n",
       "0        0.015659 -0.294400 -0.003552  ... -0.085423  0.016819  0.054813   \n",
       "1        0.788247  0.642638  0.836030  ... -0.459634  0.161808  0.352604   \n",
       "\n",
       "               15        16        17        18        19        20      score  \n",
       "cluster                                                                         \n",
       "0       -0.121067  0.078747  0.217730  0.098549 -0.448940  0.136416   4.257245  \n",
       "1        0.167175 -1.228897 -1.259259 -1.139690  1.132961 -0.563544  11.653811  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test2 = test_x.copy()\n",
    "df_test2['score'] = y_test_scores2\n",
    "df_test2['cluster'] = np.where(df_test2['score']<6.2, 0, 1)\n",
    "print(df_test2['cluster'].value_counts())\n",
    "\n",
    "df_test2.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Data Counts\n",
      "{0: 164, 1: 188}\n"
     ]
    }
   ],
   "source": [
    "#print(\"Test Data Counts\")\n",
    "#unique, counts = np.unique(y_test, return_counts=True)\n",
    "#print(dict(zip(unique, counts)))\n",
    "print(\"Predicted Data Counts\")\n",
    "unique, counts = np.unique(y_test_pred2, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 21)                462       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 25)                550       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1)                 26        \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 25)                50        \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 21)                546       \n",
      "=================================================================\n",
      "Total params: 2,096\n",
      "Trainable params: 2,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1489 samples, validate on 166 samples\n",
      "Epoch 1/100\n",
      "1489/1489 [==============================] - 0s 323us/step - loss: 59.7806 - val_loss: 39.9649\n",
      "Epoch 2/100\n",
      "1489/1489 [==============================] - 0s 116us/step - loss: 38.8793 - val_loss: 29.8876\n",
      "Epoch 3/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 30.8350 - val_loss: 23.5045\n",
      "Epoch 4/100\n",
      "1489/1489 [==============================] - 0s 124us/step - loss: 24.0369 - val_loss: 18.4367\n",
      "Epoch 5/100\n",
      "1489/1489 [==============================] - 0s 124us/step - loss: 19.7217 - val_loss: 14.6289\n",
      "Epoch 6/100\n",
      "1489/1489 [==============================] - 0s 123us/step - loss: 15.8123 - val_loss: 11.8105\n",
      "Epoch 7/100\n",
      "1489/1489 [==============================] - 0s 123us/step - loss: 13.2254 - val_loss: 9.8895\n",
      "Epoch 8/100\n",
      "1489/1489 [==============================] - 0s 114us/step - loss: 11.6906 - val_loss: 8.5249\n",
      "Epoch 9/100\n",
      "1489/1489 [==============================] - 0s 146us/step - loss: 10.1782 - val_loss: 7.5469\n",
      "Epoch 10/100\n",
      "1489/1489 [==============================] - 0s 142us/step - loss: 9.2494 - val_loss: 6.7453\n",
      "Epoch 11/100\n",
      "1489/1489 [==============================] - 0s 151us/step - loss: 8.4098 - val_loss: 6.1385\n",
      "Epoch 12/100\n",
      "1489/1489 [==============================] - 0s 151us/step - loss: 7.9535 - val_loss: 5.6449\n",
      "Epoch 13/100\n",
      "1489/1489 [==============================] - 0s 122us/step - loss: 7.2350 - val_loss: 5.2432\n",
      "Epoch 14/100\n",
      "1489/1489 [==============================] - 0s 115us/step - loss: 6.9416 - val_loss: 4.9138\n",
      "Epoch 15/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 6.3552 - val_loss: 4.6196\n",
      "Epoch 16/100\n",
      "1489/1489 [==============================] - 0s 111us/step - loss: 6.1194 - val_loss: 4.3758\n",
      "Epoch 17/100\n",
      "1489/1489 [==============================] - 0s 121us/step - loss: 5.8492 - val_loss: 4.1598\n",
      "Epoch 18/100\n",
      "1489/1489 [==============================] - 0s 117us/step - loss: 5.5978 - val_loss: 3.9744\n",
      "Epoch 19/100\n",
      "1489/1489 [==============================] - 0s 119us/step - loss: 5.2677 - val_loss: 3.8061\n",
      "Epoch 20/100\n",
      "1489/1489 [==============================] - 0s 128us/step - loss: 5.0065 - val_loss: 3.6634\n",
      "Epoch 21/100\n",
      "1489/1489 [==============================] - 0s 124us/step - loss: 4.8108 - val_loss: 3.5308\n",
      "Epoch 22/100\n",
      "1489/1489 [==============================] - 0s 119us/step - loss: 4.6637 - val_loss: 3.4097\n",
      "Epoch 23/100\n",
      "1489/1489 [==============================] - 0s 117us/step - loss: 4.4514 - val_loss: 3.3042\n",
      "Epoch 24/100\n",
      "1489/1489 [==============================] - 0s 117us/step - loss: 4.2691 - val_loss: 3.2028\n",
      "Epoch 25/100\n",
      "1489/1489 [==============================] - 0s 124us/step - loss: 4.1680 - val_loss: 3.1146\n",
      "Epoch 26/100\n",
      "1489/1489 [==============================] - 0s 112us/step - loss: 4.1557 - val_loss: 3.0347\n",
      "Epoch 27/100\n",
      "1489/1489 [==============================] - 0s 114us/step - loss: 3.8557 - val_loss: 2.9558\n",
      "Epoch 28/100\n",
      "1489/1489 [==============================] - 0s 116us/step - loss: 3.7607 - val_loss: 2.8849\n",
      "Epoch 29/100\n",
      "1489/1489 [==============================] - 0s 112us/step - loss: 3.6280 - val_loss: 2.8186\n",
      "Epoch 30/100\n",
      "1489/1489 [==============================] - 0s 119us/step - loss: 3.4746 - val_loss: 2.7578\n",
      "Epoch 31/100\n",
      "1489/1489 [==============================] - 0s 147us/step - loss: 3.4518 - val_loss: 2.6996\n",
      "Epoch 32/100\n",
      "1489/1489 [==============================] - 0s 130us/step - loss: 3.3076 - val_loss: 2.6482\n",
      "Epoch 33/100\n",
      "1489/1489 [==============================] - 0s 112us/step - loss: 3.2408 - val_loss: 2.5970\n",
      "Epoch 34/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 3.1322 - val_loss: 2.5496\n",
      "Epoch 35/100\n",
      "1489/1489 [==============================] - 0s 110us/step - loss: 3.0710 - val_loss: 2.5018\n",
      "Epoch 36/100\n",
      "1489/1489 [==============================] - 0s 110us/step - loss: 2.9980 - val_loss: 2.4634\n",
      "Epoch 37/100\n",
      "1489/1489 [==============================] - 0s 116us/step - loss: 2.9096 - val_loss: 2.4206\n",
      "Epoch 38/100\n",
      "1489/1489 [==============================] - 0s 125us/step - loss: 2.8365 - val_loss: 2.3813\n",
      "Epoch 39/100\n",
      "1489/1489 [==============================] - 0s 127us/step - loss: 2.7671 - val_loss: 2.3446\n",
      "Epoch 40/100\n",
      "1489/1489 [==============================] - 0s 141us/step - loss: 2.7032 - val_loss: 2.3104\n",
      "Epoch 41/100\n",
      "1489/1489 [==============================] - 0s 153us/step - loss: 2.6819 - val_loss: 2.2756\n",
      "Epoch 42/100\n",
      "1489/1489 [==============================] - 0s 127us/step - loss: 2.6130 - val_loss: 2.2435\n",
      "Epoch 43/100\n",
      "1489/1489 [==============================] - 0s 146us/step - loss: 2.5429 - val_loss: 2.2121\n",
      "Epoch 44/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 2.4917 - val_loss: 2.1803\n",
      "Epoch 45/100\n",
      "1489/1489 [==============================] - 0s 147us/step - loss: 2.4522 - val_loss: 2.1545\n",
      "Epoch 46/100\n",
      "1489/1489 [==============================] - 0s 151us/step - loss: 2.3925 - val_loss: 2.1263\n",
      "Epoch 47/100\n",
      "1489/1489 [==============================] - 0s 139us/step - loss: 2.3451 - val_loss: 2.0999\n",
      "Epoch 48/100\n",
      "1489/1489 [==============================] - 0s 157us/step - loss: 2.3104 - val_loss: 2.0743\n",
      "Epoch 49/100\n",
      "1489/1489 [==============================] - 0s 114us/step - loss: 2.2682 - val_loss: 2.0487\n",
      "Epoch 50/100\n",
      "1489/1489 [==============================] - 0s 141us/step - loss: 2.2445 - val_loss: 2.0249\n",
      "Epoch 51/100\n",
      "1489/1489 [==============================] - 0s 161us/step - loss: 2.1940 - val_loss: 2.0023\n",
      "Epoch 52/100\n",
      "1489/1489 [==============================] - 0s 112us/step - loss: 2.1627 - val_loss: 1.9789\n",
      "Epoch 53/100\n",
      "1489/1489 [==============================] - 0s 110us/step - loss: 2.1332 - val_loss: 1.9576\n",
      "Epoch 54/100\n",
      "1489/1489 [==============================] - 0s 129us/step - loss: 2.0892 - val_loss: 1.9362\n",
      "Epoch 55/100\n",
      "1489/1489 [==============================] - 0s 116us/step - loss: 2.0682 - val_loss: 1.9157\n",
      "Epoch 56/100\n",
      "1489/1489 [==============================] - 0s 123us/step - loss: 2.0332 - val_loss: 1.8956\n",
      "Epoch 57/100\n",
      "1489/1489 [==============================] - 0s 116us/step - loss: 2.0275 - val_loss: 1.8767\n",
      "Epoch 58/100\n",
      "1489/1489 [==============================] - 0s 115us/step - loss: 1.9849 - val_loss: 1.8577\n",
      "Epoch 59/100\n",
      "1489/1489 [==============================] - 0s 119us/step - loss: 1.9581 - val_loss: 1.8396\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1489/1489 [==============================] - 0s 119us/step - loss: 1.9270 - val_loss: 1.8214\n",
      "Epoch 61/100\n",
      "1489/1489 [==============================] - 0s 110us/step - loss: 1.9125 - val_loss: 1.8045\n",
      "Epoch 62/100\n",
      "1489/1489 [==============================] - 0s 112us/step - loss: 1.8970 - val_loss: 1.7878\n",
      "Epoch 63/100\n",
      "1489/1489 [==============================] - 0s 114us/step - loss: 1.8692 - val_loss: 1.7711\n",
      "Epoch 64/100\n",
      "1489/1489 [==============================] - 0s 115us/step - loss: 1.8463 - val_loss: 1.7547\n",
      "Epoch 65/100\n",
      "1489/1489 [==============================] - 0s 119us/step - loss: 1.8325 - val_loss: 1.7393\n",
      "Epoch 66/100\n",
      "1489/1489 [==============================] - 0s 115us/step - loss: 1.8139 - val_loss: 1.7247\n",
      "Epoch 67/100\n",
      "1489/1489 [==============================] - 0s 117us/step - loss: 1.7899 - val_loss: 1.7098\n",
      "Epoch 68/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 1.7710 - val_loss: 1.6946\n",
      "Epoch 69/100\n",
      "1489/1489 [==============================] - 0s 127us/step - loss: 1.7563 - val_loss: 1.6808\n",
      "Epoch 70/100\n",
      "1489/1489 [==============================] - 0s 112us/step - loss: 1.7369 - val_loss: 1.6666\n",
      "Epoch 71/100\n",
      "1489/1489 [==============================] - 0s 123us/step - loss: 1.7301 - val_loss: 1.6536\n",
      "Epoch 72/100\n",
      "1489/1489 [==============================] - 0s 115us/step - loss: 1.7054 - val_loss: 1.6403\n",
      "Epoch 73/100\n",
      "1489/1489 [==============================] - 0s 115us/step - loss: 1.6951 - val_loss: 1.6277\n",
      "Epoch 74/100\n",
      "1489/1489 [==============================] - 0s 112us/step - loss: 1.6787 - val_loss: 1.6146\n",
      "Epoch 75/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 1.6775 - val_loss: 1.6028\n",
      "Epoch 76/100\n",
      "1489/1489 [==============================] - 0s 114us/step - loss: 1.6495 - val_loss: 1.5901\n",
      "Epoch 77/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 1.6338 - val_loss: 1.5787\n",
      "Epoch 78/100\n",
      "1489/1489 [==============================] - 0s 121us/step - loss: 1.6283 - val_loss: 1.5675\n",
      "Epoch 79/100\n",
      "1489/1489 [==============================] - 0s 117us/step - loss: 1.6147 - val_loss: 1.5557\n",
      "Epoch 80/100\n",
      "1489/1489 [==============================] - 0s 112us/step - loss: 1.5895 - val_loss: 1.5444\n",
      "Epoch 81/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 1.5824 - val_loss: 1.5340\n",
      "Epoch 82/100\n",
      "1489/1489 [==============================] - 0s 117us/step - loss: 1.5684 - val_loss: 1.5235\n",
      "Epoch 83/100\n",
      "1489/1489 [==============================] - 0s 115us/step - loss: 1.5629 - val_loss: 1.5133\n",
      "Epoch 84/100\n",
      "1489/1489 [==============================] - 0s 110us/step - loss: 1.5603 - val_loss: 1.5037\n",
      "Epoch 85/100\n",
      "1489/1489 [==============================] - 0s 114us/step - loss: 1.5437 - val_loss: 1.4931\n",
      "Epoch 86/100\n",
      "1489/1489 [==============================] - 0s 115us/step - loss: 1.5241 - val_loss: 1.4836\n",
      "Epoch 87/100\n",
      "1489/1489 [==============================] - 0s 110us/step - loss: 1.5199 - val_loss: 1.4743\n",
      "Epoch 88/100\n",
      "1489/1489 [==============================] - 0s 119us/step - loss: 1.5093 - val_loss: 1.4645\n",
      "Epoch 89/100\n",
      "1489/1489 [==============================] - 0s 128us/step - loss: 1.5042 - val_loss: 1.4561\n",
      "Epoch 90/100\n",
      "1489/1489 [==============================] - 0s 115us/step - loss: 1.4890 - val_loss: 1.4469\n",
      "Epoch 91/100\n",
      "1489/1489 [==============================] - 0s 109us/step - loss: 1.4783 - val_loss: 1.4379\n",
      "Epoch 92/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 1.4730 - val_loss: 1.4291\n",
      "Epoch 93/100\n",
      "1489/1489 [==============================] - 0s 111us/step - loss: 1.4581 - val_loss: 1.4209\n",
      "Epoch 94/100\n",
      "1489/1489 [==============================] - 0s 118us/step - loss: 1.4474 - val_loss: 1.4127\n",
      "Epoch 95/100\n",
      "1489/1489 [==============================] - 0s 113us/step - loss: 1.4405 - val_loss: 1.4049\n",
      "Epoch 96/100\n",
      "1489/1489 [==============================] - 0s 116us/step - loss: 1.4327 - val_loss: 1.3967\n",
      "Epoch 97/100\n",
      "1489/1489 [==============================] - 0s 124us/step - loss: 1.4214 - val_loss: 1.3888\n",
      "Epoch 98/100\n",
      "1489/1489 [==============================] - 0s 114us/step - loss: 1.4127 - val_loss: 1.3814\n",
      "Epoch 99/100\n",
      "1489/1489 [==============================] - 0s 111us/step - loss: 1.4116 - val_loss: 1.3740\n",
      "Epoch 100/100\n",
      "1489/1489 [==============================] - 0s 125us/step - loss: 1.4008 - val_loss: 1.3664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoEncoder(batch_size=32, contamination=0.1, dropout_rate=0.2, epochs=100,\n",
       "      hidden_activation='relu', hidden_neurons=[25, 1, 25],\n",
       "      l2_regularizer=0.1,\n",
       "      loss=<function mean_squared_error at 0x0000013614087288>,\n",
       "      optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "      random_state=None, validation_size=0.1, verbose=1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Build the model\n",
    "clf3 = AutoEncoder(hidden_neurons =[\n",
    "  #  100, 50,\n",
    "    25, 1, 25\n",
    "  #  , 50, 100\n",
    "]\n",
    "                 #  ,contamination = contam\n",
    "                   ,epochs=epochs\n",
    "                  )\n",
    "clf3.fit(clean_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Predict the anomaly scores\n",
    "y_train_pred3 = clf3.labels_\n",
    "y_test_scores3 = clf3.decision_function(test_x)\n",
    "#print(y_test_scores.shape)\n",
    "y_test_scores3 = pd.Series(y_test_scores3)\n",
    "y_train_scores3 = clf3.decision_scores_  # raw outlier scores\n",
    "y_test_pred3 = clf3.predict(test_x)\n",
    "\n",
    "\n",
    "# Step 2: Determine the cut point\n",
    "plt.hist(y_test_scores3, bins='auto')  \n",
    "plt.title(\"Histogram with Model Clf3 Anomaly Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "On Test Data:\n",
      "clf_3 ROC:0.9284, precision @ rank n:0.8629\n"
     ]
    }
   ],
   "source": [
    "# evaluate and print the results\n",
    "#print(\"\\nOn Training Data:\")\n",
    "#evaluate_print('clf_3', y_train3, y_train_scores3)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print('clf_3', test_y, y_test_scores3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Data Counts\n",
      "{0: 164, 1: 188}\n"
     ]
    }
   ],
   "source": [
    "#print(\"Test Data Counts\")\n",
    "#unique, counts = np.unique(y_test3, return_counts=True)\n",
    "#print(dict(zip(unique, counts)))\n",
    "print(\"Predicted Data Counts\")\n",
    "unique, counts = np.unique(y_test_pred3, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    186\n",
      "0    166\n",
      "Name: cluster, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038060</td>\n",
       "      <td>-0.101065</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>-0.541590</td>\n",
       "      <td>-0.422266</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>-0.294400</td>\n",
       "      <td>-0.003552</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085423</td>\n",
       "      <td>0.016819</td>\n",
       "      <td>0.054813</td>\n",
       "      <td>-0.121067</td>\n",
       "      <td>0.078747</td>\n",
       "      <td>0.217730</td>\n",
       "      <td>0.098549</td>\n",
       "      <td>-0.448940</td>\n",
       "      <td>0.136416</td>\n",
       "      <td>4.260661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.076442</td>\n",
       "      <td>-0.693276</td>\n",
       "      <td>0.669675</td>\n",
       "      <td>-0.211297</td>\n",
       "      <td>0.667162</td>\n",
       "      <td>0.472409</td>\n",
       "      <td>1.760327</td>\n",
       "      <td>0.788247</td>\n",
       "      <td>0.642638</td>\n",
       "      <td>0.836030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.459634</td>\n",
       "      <td>0.161808</td>\n",
       "      <td>0.352604</td>\n",
       "      <td>0.167175</td>\n",
       "      <td>-1.228897</td>\n",
       "      <td>-1.259259</td>\n",
       "      <td>-1.139690</td>\n",
       "      <td>1.132961</td>\n",
       "      <td>-0.563544</td>\n",
       "      <td>11.652077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "cluster                                                                         \n",
       "0        0.038060 -0.101065  0.008917 -0.541590 -0.422266 -0.061401 -0.278295   \n",
       "1       -0.076442 -0.693276  0.669675 -0.211297  0.667162  0.472409  1.760327   \n",
       "\n",
       "                7         8         9  ...        12        13        14  \\\n",
       "cluster                                ...                                 \n",
       "0        0.015659 -0.294400 -0.003552  ... -0.085423  0.016819  0.054813   \n",
       "1        0.788247  0.642638  0.836030  ... -0.459634  0.161808  0.352604   \n",
       "\n",
       "               15        16        17        18        19        20      score  \n",
       "cluster                                                                         \n",
       "0       -0.121067  0.078747  0.217730  0.098549 -0.448940  0.136416   4.260661  \n",
       "1        0.167175 -1.228897 -1.259259 -1.139690  1.132961 -0.563544  11.652077  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test3 = test_x.copy()\n",
    "df_test3['score'] = y_test_scores3\n",
    "df_test3['cluster'] = np.where(df_test3['score']<6.2, 0, 1)\n",
    "print(df_test3['cluster'].value_counts())\n",
    "\n",
    "df_test3.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Put all the predictions in a data frame\n",
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "\n",
    "# Put all the predictions in a data frame\n",
    "train_scores = pd.DataFrame({'clf1': clf1.decision_scores_,\n",
    "                             'clf2': clf2.decision_scores_,\n",
    "                             'clf3': clf3.decision_scores_\n",
    "                            })\n",
    "\n",
    "test_scores  = pd.DataFrame({'clf1': clf1.decision_function(test_x),\n",
    "                             'clf2': clf2.decision_function(test_x),\n",
    "                             'clf3': clf3.decision_function(test_x) \n",
    "                            })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although we did standardization before, it was for the variables.\n",
    "# Now we do the standardization for the decision scores\n",
    "train_scores_norm, test_scores_norm = standardizer(train_scores,test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combination by average\n",
    "y_by_average = average(test_scores_norm)\n",
    "  \n",
    "\n",
    "  \n",
    "plt.hist(y_by_average, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by average\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# x, y = np.random.rand(2, 100) * 4\n",
    "\n",
    "# hist, xedges, yedges = np.histogram2d(x_test[0], y_by_maximization, bins=10, range=[[-2, 2.5], [-2, 2.5]])\n",
    "\n",
    "# # Construct arrays for the anchor positions of the 16 bars.\n",
    "# xpos, ypos = np.meshgrid(xedges[:-1] + 0.25, yedges[:-1] + 0.25, indexing=\"ij\")\n",
    "# xpos = xpos.ravel()\n",
    "# ypos = ypos.ravel()\n",
    "# zpos = 0\n",
    "\n",
    "# # Construct arrays with the dimensions for the 16 bars.\n",
    "# dx = dy = 0.5 * np.ones_like(zpos)\n",
    "# dz = hist.ravel()\n",
    "\n",
    "# ax.bar3d(xpos, ypos, zpos, dx, dy, dz, zsort='average')\n",
    "# ax.view_init(azim=-220,elev=36)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    201\n",
      "0    151\n",
      "Name: y_by_average_cluster, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>y_by_average_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_average_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.074275</td>\n",
       "      <td>-0.077874</td>\n",
       "      <td>-0.033992</td>\n",
       "      <td>-0.591799</td>\n",
       "      <td>-0.464675</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>-0.002669</td>\n",
       "      <td>-0.313149</td>\n",
       "      <td>-0.033443</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.104046</td>\n",
       "      <td>-0.045188</td>\n",
       "      <td>0.032580</td>\n",
       "      <td>-0.123036</td>\n",
       "      <td>0.020792</td>\n",
       "      <td>0.161824</td>\n",
       "      <td>0.034188</td>\n",
       "      <td>-0.461654</td>\n",
       "      <td>0.112437</td>\n",
       "      <td>-0.082073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016494</td>\n",
       "      <td>-0.666503</td>\n",
       "      <td>0.652601</td>\n",
       "      <td>-0.198226</td>\n",
       "      <td>0.617721</td>\n",
       "      <td>0.432572</td>\n",
       "      <td>1.608191</td>\n",
       "      <td>0.744360</td>\n",
       "      <td>0.586795</td>\n",
       "      <td>0.795830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417717</td>\n",
       "      <td>0.197571</td>\n",
       "      <td>0.347084</td>\n",
       "      <td>0.147143</td>\n",
       "      <td>-1.087774</td>\n",
       "      <td>-1.107037</td>\n",
       "      <td>-0.998933</td>\n",
       "      <td>1.024459</td>\n",
       "      <td>-0.493294</td>\n",
       "      <td>3.988343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             0         1         2         3         4  \\\n",
       "y_by_average_cluster                                                     \n",
       "0                    -0.074275 -0.077874 -0.033992 -0.591799 -0.464675   \n",
       "1                     0.016494 -0.666503  0.652601 -0.198226  0.617721   \n",
       "\n",
       "                             5         6         7         8         9  ...  \\\n",
       "y_by_average_cluster                                                    ...   \n",
       "0                    -0.061401 -0.278295 -0.002669 -0.313149 -0.033443  ...   \n",
       "1                     0.432572  1.608191  0.744360  0.586795  0.795830  ...   \n",
       "\n",
       "                            12        13        14        15        16  \\\n",
       "y_by_average_cluster                                                     \n",
       "0                    -0.104046 -0.045188  0.032580 -0.123036  0.020792   \n",
       "1                    -0.417717  0.197571  0.347084  0.147143 -1.087774   \n",
       "\n",
       "                            17        18        19        20  \\\n",
       "y_by_average_cluster                                           \n",
       "0                     0.161824  0.034188 -0.461654  0.112437   \n",
       "1                    -1.107037 -0.998933  1.024459 -0.493294   \n",
       "\n",
       "                      y_by_average_score  \n",
       "y_by_average_cluster                      \n",
       "0                              -0.082073  \n",
       "1                               3.988343  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(test_x)\n",
    "df_test['y_by_average_score'] = y_by_average\n",
    "df_test['y_by_average_cluster'] = np.where(df_test['y_by_average_score']<0.8, 0, 1)\n",
    "print(df_test['y_by_average_cluster'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "df_test.groupby('y_by_average_cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    313\n",
      "1     39\n",
      "Name: y_by_maximization_cluster, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>y_by_average_score</th>\n",
       "      <th>y_by_average_cluster</th>\n",
       "      <th>y_by_maximization_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_maximization_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.007934</td>\n",
       "      <td>-0.386839</td>\n",
       "      <td>0.216292</td>\n",
       "      <td>-0.393583</td>\n",
       "      <td>0.064271</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>0.402955</td>\n",
       "      <td>0.406558</td>\n",
       "      <td>0.124881</td>\n",
       "      <td>0.547319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125855</td>\n",
       "      <td>-0.057044</td>\n",
       "      <td>-0.326388</td>\n",
       "      <td>-0.371728</td>\n",
       "      <td>-0.361665</td>\n",
       "      <td>-0.021943</td>\n",
       "      <td>-0.086271</td>\n",
       "      <td>1.274758</td>\n",
       "      <td>0.517572</td>\n",
       "      <td>1.276558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.138890</td>\n",
       "      <td>-0.631936</td>\n",
       "      <td>1.495910</td>\n",
       "      <td>-0.154195</td>\n",
       "      <td>0.868692</td>\n",
       "      <td>2.484460</td>\n",
       "      <td>3.976899</td>\n",
       "      <td>0.563098</td>\n",
       "      <td>0.809555</td>\n",
       "      <td>-0.420487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904893</td>\n",
       "      <td>0.739796</td>\n",
       "      <td>-2.906241</td>\n",
       "      <td>-2.095595</td>\n",
       "      <td>-2.113389</td>\n",
       "      <td>3.668582</td>\n",
       "      <td>-1.414649</td>\n",
       "      <td>10.006784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.011691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0         1         2         3         4  \\\n",
       "y_by_maximization_cluster                                                     \n",
       "0                         -0.007934 -0.386839  0.216292 -0.393583  0.064271   \n",
       "1                         -0.138890 -0.631936  1.495910 -0.154195  0.868692   \n",
       "\n",
       "                                  5         6         7         8         9  \\\n",
       "y_by_maximization_cluster                                                     \n",
       "0                         -0.061401  0.402955  0.406558  0.124881  0.547319   \n",
       "1                          2.484460  3.976899  0.563098  0.809555 -0.420487   \n",
       "\n",
       "                           ...        14        15        16        17  \\\n",
       "y_by_maximization_cluster  ...                                           \n",
       "0                          ...  0.125855 -0.057044 -0.326388 -0.371728   \n",
       "1                          ...  0.904893  0.739796 -2.906241 -2.095595   \n",
       "\n",
       "                                 18        19        20  y_by_average_score  \\\n",
       "y_by_maximization_cluster                                                     \n",
       "0                         -0.361665 -0.021943 -0.086271            1.274758   \n",
       "1                         -2.113389  3.668582 -1.414649           10.006784   \n",
       "\n",
       "                           y_by_average_cluster  y_by_maximization_score  \n",
       "y_by_maximization_cluster                                                 \n",
       "0                                      0.517572                 1.276558  \n",
       "1                                      1.000000                10.011691  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combination by max\n",
    "y_by_maximization = maximization(test_scores_norm)\n",
    "             \n",
    "\n",
    "plt.hist(y_by_maximization, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by max\")\n",
    "plt.show()\n",
    "\n",
    "df_test = pd.DataFrame(test_x)\n",
    "df_test['y_by_maximization_score'] = y_by_maximization\n",
    "df_test['y_by_maximization_cluster'] = np.where(df_test['y_by_maximization_score']<5, 0, 1)\n",
    "print(df_test['y_by_maximization_cluster'].value_counts())\n",
    "\n",
    "df_test.groupby('y_by_maximization_cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    220\n",
      "0    132\n",
      "Name: y_by_aom_cluster, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>y_by_average_score</th>\n",
       "      <th>y_by_average_cluster</th>\n",
       "      <th>y_by_maximization_score</th>\n",
       "      <th>y_by_maximization_cluster</th>\n",
       "      <th>y_by_aom_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_aom_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.169044</td>\n",
       "      <td>-0.032391</td>\n",
       "      <td>-0.027043</td>\n",
       "      <td>-0.538326</td>\n",
       "      <td>-0.455024</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>-0.278295</td>\n",
       "      <td>-0.179282</td>\n",
       "      <td>-0.264812</td>\n",
       "      <td>-0.136634</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020974</td>\n",
       "      <td>0.114271</td>\n",
       "      <td>-0.013545</td>\n",
       "      <td>-0.461089</td>\n",
       "      <td>0.162505</td>\n",
       "      <td>-0.186212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.184254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.186418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.065517</td>\n",
       "      <td>-0.642957</td>\n",
       "      <td>0.589134</td>\n",
       "      <td>-0.264301</td>\n",
       "      <td>0.518450</td>\n",
       "      <td>0.389911</td>\n",
       "      <td>1.445267</td>\n",
       "      <td>0.785812</td>\n",
       "      <td>0.480070</td>\n",
       "      <td>0.786125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.966974</td>\n",
       "      <td>-0.968922</td>\n",
       "      <td>-0.881069</td>\n",
       "      <td>0.895774</td>\n",
       "      <td>-0.471022</td>\n",
       "      <td>3.699290</td>\n",
       "      <td>0.913636</td>\n",
       "      <td>3.701546</td>\n",
       "      <td>0.177273</td>\n",
       "      <td>3.698927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         0         1         2         3         4         5  \\\n",
       "y_by_aom_cluster                                                               \n",
       "0                -0.169044 -0.032391 -0.027043 -0.538326 -0.455024 -0.061401   \n",
       "1                 0.065517 -0.642957  0.589134 -0.264301  0.518450  0.389911   \n",
       "\n",
       "                         6         7         8         9  ...        16  \\\n",
       "y_by_aom_cluster                                          ...             \n",
       "0                -0.278295 -0.179282 -0.264812 -0.136634  ... -0.020974   \n",
       "1                 1.445267  0.785812  0.480070  0.786125  ... -0.966974   \n",
       "\n",
       "                        17        18        19        20  y_by_average_score  \\\n",
       "y_by_aom_cluster                                                               \n",
       "0                 0.114271 -0.013545 -0.461089  0.162505           -0.186212   \n",
       "1                -0.968922 -0.881069  0.895774 -0.471022            3.699290   \n",
       "\n",
       "                  y_by_average_cluster  y_by_maximization_score  \\\n",
       "y_by_aom_cluster                                                  \n",
       "0                             0.000000                -0.184254   \n",
       "1                             0.913636                 3.701546   \n",
       "\n",
       "                  y_by_maximization_cluster  y_by_aom_score  \n",
       "y_by_aom_cluster                                             \n",
       "0                                  0.000000       -0.186418  \n",
       "1                                  0.177273        3.698927  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combination by aom\n",
    "y_by_aom = median(test_scores_norm)\n",
    "\n",
    "\n",
    "plt.hist(y_by_aom, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Average of Maximum\")\n",
    "plt.show()\n",
    "\n",
    "df_test = pd.DataFrame(test_x)\n",
    "df_test['y_by_aom_score'] = y_by_aom\n",
    "df_test['y_by_aom_cluster'] = np.where(df_test['y_by_aom_score']<0.5, 0, 1)\n",
    "print(df_test['y_by_aom_cluster'].value_counts())\n",
    "\n",
    "\n",
    "df_test.groupby('y_by_aom_cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 663, 1: 70}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test_pred, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 663, 1.0: 70}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
