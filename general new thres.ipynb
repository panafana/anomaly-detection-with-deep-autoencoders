{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import ipympl\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.io\n",
    "import hdf5storage\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "#import matplotlib\n",
    "#matplotlib.use('nbagg')\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.mstats import gmean \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from tensorflow.python.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from tensorflow.python.keras.models import Model\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Average\n",
    "from tensorflow.keras.models import Model\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import TensorBoard\n",
    "#indicate folder to save, plus other options\n",
    "tensorboard = TensorBoard(log_dir='./logs/run', histogram_freq=0,\n",
    "    write_graph=True, write_images=False)  \n",
    "def sigmoid(X):\n",
    "   return 1/(1+np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "noise_amount = 1\n",
    "\n",
    "sparse_g = False\n",
    "sparsity_g = 1\n",
    "kernel_g = 0\n",
    "bias_g = 0\n",
    "dropout_g = False\n",
    "dropoutRate_g = 0.1\n",
    "message = \"\"\n",
    "\n",
    "simple = True\n",
    "contracting = False\n",
    "sparse = False\n",
    "sparseDropout = False\n",
    "denoising = False\n",
    "\n",
    "if(simple):\n",
    "    sparse_g = False\n",
    "    sparsity_g = 1\n",
    "    kernel_g = 0\n",
    "    bias_g = 0\n",
    "    dropout_g = False\n",
    "    dropoutRate_g = 0.1\n",
    "    message = \"simple model\"\n",
    "elif(contracting):\n",
    "    sparse_g = False\n",
    "    sparsity_g = 1\n",
    "    kernel_g = 0.01\n",
    "    bias_g = 0.01\n",
    "    dropout_g = False\n",
    "    dropoutRate_g = 0.1\n",
    "    message = \"contracting model\"\n",
    "elif(sparse):\n",
    "    sparse_g = True\n",
    "    sparsity_g = 1\n",
    "    kernel_g = 0\n",
    "    bias_g = 0\n",
    "    dropout_g = False\n",
    "    dropoutRate_g = 0.1\n",
    "    message = \"sparse model\"\n",
    "elif(sparseDropout):\n",
    "    sparse_g = True\n",
    "    sparsity_g = 1\n",
    "    kernel_g = 0\n",
    "    bias_g = 0\n",
    "    dropout_g = True\n",
    "    dropoutRate_g = 0.1\n",
    "    message = \"sparseDropout model\"\n",
    "elif(denoising):\n",
    "    sparse_g = False\n",
    "    sparsity_g = 1\n",
    "    kernel_g = 0\n",
    "    bias_g = 0\n",
    "    dropout_g = False\n",
    "    dropoutRate_g = 0.1\n",
    "    message = \"denoising model\"\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average2(lst): \n",
    "    return sum(lst) / len(lst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "    return list(roc_t['threshold']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(inputs,table,kernel,bias,sparse,sparsity,dropout,dropoutRate,shape):\n",
    "    for i in range(len(table)):\n",
    "        if(i==0):\n",
    "            if(sparse):\n",
    "                if(dropout):\n",
    "                    encoded = Dense(table[i], activation='relu'\n",
    "                    #,activity_regularizer=regularizers.l1(sparsity)\n",
    "                     )(inputs)\n",
    "                    encoded = Dropout(dropoutRate)(encoded)\n",
    "                else:\n",
    "                    encoded = Dense(table[i], activation='relu'\n",
    "                    #,activity_regularizer=regularizers.l1(sparsity)\n",
    "                     )(inputs)\n",
    "            elif(kernel==0):\n",
    "                encoded = Dense(table[i], activation='relu')(inputs)\n",
    "            else:\n",
    "                encoded = Dense(table[i], activation='relu'\n",
    "                    ,kernel_regularizer=l2(kernel) \n",
    "                    ,bias_regularizer=l2(bias)\n",
    "                    )(inputs)\n",
    "        elif(i==math.floor(len(table)/2)):\n",
    "            if(sparse):\n",
    "                if(dropout):\n",
    "                    encoded = Dense(table[i], activation='relu'\n",
    "                    ,activity_regularizer=regularizers.l1(sparsity)\n",
    "                     )(encoded)\n",
    "                    encoded = Dropout(dropoutRate)(encoded)\n",
    "                else:\n",
    "                    encoded = Dense(table[i], activation='relu'\n",
    "                    ,activity_regularizer=regularizers.l1(sparsity)\n",
    "                     )(encoded)\n",
    "            elif(kernel==0):\n",
    "                encoded = Dense(table[i], activation='relu')(encoded)\n",
    "            else:\n",
    "                encoded = Dense(table[i], activation='relu'\n",
    "                    ,kernel_regularizer=l2(kernel) \n",
    "                    ,bias_regularizer=l2(bias)\n",
    "                    )(encoded)    \n",
    "        else:\n",
    "            if(sparse):\n",
    "                if(dropout):\n",
    "                    encoded = Dense(table[i], activation='relu'\n",
    "                    #,activity_regularizer=regularizers.l1(sparsity)\n",
    "                     )(encoded)\n",
    "                    encoded = Dropout(dropoutRate)(encoded)\n",
    "                else:\n",
    "                    encoded = Dense(table[i], activation='relu'\n",
    "                    #,activity_regularizer=regularizers.l1(sparsity)\n",
    "                     )(encoded)\n",
    "            elif(kernel==0):\n",
    "                encoded = Dense(table[i], activation='relu')(encoded)\n",
    "            else:\n",
    "                encoded = Dense(table[i], activation='relu'\n",
    "                    ,kernel_regularizer=l2(kernel) \n",
    "                    ,bias_regularizer=l2(bias)\n",
    "                )(encoded)\n",
    "    \n",
    "    \n",
    "    if(sparse):\n",
    "        if(dropout):\n",
    "            encodedL = Dense(shape, activation='tanh'\n",
    "            #,activity_regularizer=regularizers.l1(sparsity)\n",
    "                )(encoded)\n",
    "            encodedL = Dropout(dropoutRate)(encodedL)\n",
    "        else:\n",
    "            encodedL = Dense(shape, activation='tanh'\n",
    "            #,activity_regularizer=regularizers.l1(sparsity)\n",
    "                )(encoded)\n",
    "    elif(kernel==0):\n",
    "        encodedL = Dense(shape, activation='tanh')(encoded)\n",
    "    else:\n",
    "        encodedL = Dense(shape, activation='tanh'\n",
    "            ,kernel_regularizer=l2(kernel) \n",
    "            ,bias_regularizer=l2(bias)\n",
    "            )(encoded)        \n",
    "    model = Model(inputs, encodedL)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    #model.summary()\n",
    "    return model , encodedL\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95156, 3)\n",
      "(95156,)\n",
      "Total number of outliers in Dataset\n",
      "30\n",
      "Contamination in Dataset\n",
      "0.0003152717642607928\n",
      "(95126, 3)\n",
      "test_data.shape\n",
      "(60, 4)\n",
      "train_data.shape\n",
      "(95096, 3)\n",
      "(47593,)\n",
      "Number of Outliers in Test\n",
      "30\n",
      "Number of Inliers in Test\n",
      "47563\n"
     ]
    }
   ],
   "source": [
    "percentChosen = 0.01\n",
    "#file_name = \"cardio.mat\"\n",
    "#file_name = \"ionosphere.mat\"\n",
    "#file_name = \"satellite.mat\"\n",
    "#file_name = \"shuttle.mat\"\n",
    "file_name = \"smtp.mat\"\n",
    "#file_name = \"breastw.mat\"\n",
    "#file_name = \"cover.mat\"\n",
    "#file_name = \"mnist.mat\"\n",
    "mat = hdf5storage.loadmat('smtp.mat')\n",
    "#file_name = \"wine.mat\"\n",
    "#file_name = \"arrhythmia.mat\"\n",
    "#file_name = \"thyroid.mat\"\n",
    "#file_name = \"musk.mat\"\n",
    "#file_name = \"satimage-2.mat\"\n",
    "#file_name = \"mammography.mat\"\n",
    "#file_name = \"glass.mat\"\n",
    "#file_name = \"pendigits.mat\"\n",
    "#file_name = \"pima.mat\"\n",
    "#file_name = \"wbc.mat\"\n",
    "#file_name = \"vertebral.mat\"\n",
    "\n",
    "#mat = scipy.io.loadmat(file_name)\n",
    "message = \"best models percent = \"+str(percentChosen)+\" \"+file_name   \n",
    "epochs = 200\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
    "#callbacks = []\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=1),tensorboard]\n",
    "Xtemp = mat['X']\n",
    "ytemp = mat['y']\n",
    "X = np.array(Xtemp)\n",
    "y = np.array(ytemp)\n",
    "print(X.shape)\n",
    "y = y.reshape(X.shape[0],)\n",
    "print(y.shape)\n",
    "outliers = 0\n",
    "clean_data = []\n",
    "contam_data = []\n",
    "\n",
    "for i in range(y.shape[0]):\n",
    "    if(y[i]==1.0):\n",
    "        outliers+=1;\n",
    "        contam_data.append(X[i])\n",
    "    else:\n",
    "        clean_data.append(X[i])\n",
    "      \n",
    "\n",
    "print(\"Total number of outliers in Dataset\")        \n",
    "print(outliers)\n",
    "contam = outliers/y.shape[0]\n",
    "print(\"Contamination in Dataset\")\n",
    "print(contam)\n",
    "clean_data = np.array(clean_data)\n",
    "contam_data = np.array(contam_data)\n",
    "print(clean_data.shape)\n",
    "clean_data2 = clean_data.copy()\n",
    "contam_data2 = contam_data.copy()\n",
    "# test_data_full = []\n",
    "# for i in range(176):\n",
    "#     test_data_full.append(contam_data[i])\n",
    "# one = np.ones(176)\n",
    "# test_data_full = np.array(test_data_full)\n",
    "\n",
    "# test_data_full= np.append(test_data_full,one,1)\n",
    "\n",
    "# print(test_data_full.shape)\n",
    "\n",
    "Xy = X.copy()\n",
    "Xy.shape\n",
    "Xy = np.array(Xy)\n",
    "Xy = np.insert(Xy, X.shape[1], y, axis=1)\n",
    "#print(Xy.shape)\n",
    "count = outliers\n",
    "count2 = outliers\n",
    "\n",
    "\n",
    "    \n",
    "#selecting all the outliers from dataset\n",
    "test_data =[]\n",
    "for i in range(Xy.shape[0]):\n",
    "    if(Xy[i,X.shape[1]]==1 and count>0):\n",
    "        test_data.append(Xy[i])\n",
    "        count = count-1\n",
    "    \n",
    "test_data = np.array(test_data)\n",
    "\n",
    "#selecting random inliers from train data and removing them\n",
    "indexes = np.random.choice(clean_data.shape[0], outliers, replace=False)\n",
    "\n",
    "random_clean_data = clean_data[indexes, :]\n",
    "random_clean_data = np.hstack((random_clean_data, np.zeros((random_clean_data.shape[0], 1), dtype=random_clean_data.dtype)))\n",
    "\n",
    "indexes = indexes.reshape(outliers,1)\n",
    "\n",
    "#deleting the values from train data\n",
    "clean_data = np.delete(clean_data,indexes,0)\n",
    "#print(clean_data)                \n",
    "test_data = np.append(test_data,random_clean_data,axis=0)\n",
    "np.random.shuffle(test_data)\n",
    "\n",
    "print(\"test_data.shape\")\n",
    "print(test_data.shape)\n",
    "#print(test_data.shape)\n",
    "test_x = test_data[:,:test_data.shape[1]-1]\n",
    "test_y = test_data[:,test_data.shape[1]-1]\n",
    "#print(test_x.shape)\n",
    "#print(test_y.shape)\n",
    "test_x = pd.DataFrame(test_x)\n",
    "test_y = pd.DataFrame(test_y)\n",
    "\n",
    "print(\"train_data.shape\")                    \n",
    "print(clean_data.shape)    \n",
    "X_train, X_test = train_test_split(clean_data2,  test_size=0.5)\n",
    "X_test = np.append(X_test,np.zeros((X_test.shape[0],1),dtype='float64'),axis=1)\n",
    "\n",
    "inliers_test = X_test.shape[0]\n",
    "\n",
    "contam_new = contam_data2.shape[0]/X_test.shape[0]\n",
    "contam_data3 = np.append(contam_data2,np.ones((contam_data2.shape[0],1),dtype='float64'),axis=1)\n",
    "outliers_test = contam_data3.shape[0]\n",
    "X_test = np.append(X_test,contam_data3,axis=0)\n",
    "np.random.shuffle(X_test)\n",
    "Xy_test = X_test.copy\n",
    "y_test = X_test[:,X_test.shape[1]-1]\n",
    "#print(y_test.shape)\n",
    "X_test = np.delete(X_test,X_test.shape[1]-1,axis=1)\n",
    "#print(X_test.shape)\n",
    "\n",
    "X_test_np = X_test\n",
    "y_test_np = y_test\n",
    "print(y_test.shape)\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "print(\"Number of Outliers in Test\")\n",
    "print(outliers_test)\n",
    "print(\"Number of Inliers in Test\")\n",
    "print(inliers_test)\n",
    "#print(X_train)\n",
    "#print(X_train.mean())\n",
    "#print(pd.DataFrame(X_train).describe())\n",
    "X_train_noisy = X_train\n",
    "#noisy data\n",
    "#print(X_train_noisy)\n",
    "def apply_noise(col):\n",
    "    #print(col.mean())\n",
    "    mu, sigma = 0, abs(col.mean())\n",
    "    noise = np.random.normal(mu, sigma, col.shape[0]) \n",
    "    for i  in range(col.shape[0]):\n",
    "        col[i] += noise_amount*noise[i]\n",
    "\n",
    "    return col\n",
    "#print(X_train)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "clean_data = scaler.fit_transform(clean_data)\n",
    "test_x  = scaler.transform(test_x)\n",
    "clean_data = pd.DataFrame(clean_data)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "X_train_noisy = np.apply_along_axis(apply_noise, 0, X_train)\n",
    "#clean_data = pd.DataFrame(clean_data)\n",
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clean_data[1].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results\n",
      "performanceIndividual [0.9993696552013952, 0.9995797701342635, 0.9994537011745425, 0.9996428046141239, 0.9995587586409767, 0.9996428046141239]\n",
      "-0.5532069560032691\n",
      "-0.8795133476503227\n",
      "-0.8839383293385019\n",
      "-0.878008971480546\n",
      "-0.5533015731586003\n",
      "-0.5772024057223595\n",
      "values\n",
      "network vote_result\n",
      "performance  0.9996428046141239\n",
      "performance Distance  0.994873195638014\n",
      "performance Distance positive negative votes  0.9959447817956422\n",
      "performance DistanceP  0.9996428046141239\n",
      "performance DistanceP2  0.9996428046141239\n",
      "\n",
      "0\n",
      "--- 318.6543309688568 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for k in range(iterations): \n",
    "    \n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    start_time = time.time()\n",
    "    models = []\n",
    "    outs = []\n",
    "    mses = []\n",
    "    msesTrain = []\n",
    "    aucs = []\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "    \n",
    "#     table = [5000,2000,1000,500,300,200,100,70,40,10,40,70,100,200,300,500,1000,2000,5000]\n",
    "\n",
    "#     simpleAutoencoder7,simpleAutoencoder_out7 = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "#     models.append(simpleAutoencoder7)\n",
    "#     outs.append(simpleAutoencoder_out7)\n",
    "    \n",
    "#     table = [2000,1000,500,300,200,100,70,40,10,40,70,100,200,300,500,1000,2000]\n",
    "\n",
    "#     simpleAutoencoder6,simpleAutoencoder_out6 = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "#     models.append(simpleAutoencoder6)\n",
    "#     outs.append(simpleAutoencoder_out6)\n",
    "    \n",
    "#     table = [500,300,200,100,70,40,10,40,70,100,200,300,500]\n",
    "\n",
    "#     simpleAutoencoder5,simpleAutoencoder_out5 = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "#     models.append(simpleAutoencoder5)\n",
    "#     outs.append(simpleAutoencoder_out5)\n",
    "    \n",
    "#     table = [200,150,100,70,40,10,40,70,100,150,200]\n",
    "\n",
    "#     simpleAutoencoder,simpleAutoencoder_out = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "#     models.append(simpleAutoencoder)\n",
    "#     outs.append(simpleAutoencoder_out)\n",
    "    \n",
    "#     table = [150,100,70,40,10,40,70,100,150]\n",
    "\n",
    "#     simpleAutoencoder2,simpleAutoencoder_out2 = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "#     models.append(simpleAutoencoder2)\n",
    "#     outs.append(simpleAutoencoder_out2)\n",
    "\n",
    "#     table = [100,70,40,10,40,70,100]\n",
    "\n",
    "#     simpleAutoencoder3,simpleAutoencoder_out3 = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "#     models.append(simpleAutoencoder3)\n",
    "#     outs.append(simpleAutoencoder_out3)\n",
    "    \n",
    "#     table = [70,40,10,40,70]\n",
    "\n",
    "#     simpleAutoencoder8,simpleAutoencoder_out8 = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "#     models.append(simpleAutoencoder8)\n",
    "#     outs.append(simpleAutoencoder_out8)\n",
    "    \n",
    "#     table = [40,10,40]\n",
    "\n",
    "#     simpleAutoencoder4,simpleAutoencoder_out4 = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "#     models.append(simpleAutoencoder4)\n",
    "#     outs.append(simpleAutoencoder_out4)\n",
    "    \n",
    "    \n",
    "#     #cardio best archs\n",
    "    \n",
    "    #simple model\n",
    "    #table = [30,20,10,20,30]\n",
    "    #table = [100,70,40,10,40,70,100]\n",
    "    table = [2000,1000,500,300,200,100,70,40,10,40,70,100,200,300,500,1000,2000]\n",
    "    #table = [5000,2000,1000,500,300,200,100,70,40,10,40,70,100,200,300,500,1000,2000,5000]\n",
    "    simpleAutoencoder1,simpleAutoencoder_out1 = createModel(inputs,table,0,0,False,1,False,0.1,X.shape[1])\n",
    "    models.append(simpleAutoencoder1)\n",
    "    outs.append(simpleAutoencoder_out1)\n",
    "    \n",
    "    #contracting model\n",
    "\n",
    "\n",
    "    simpleAutoencoder3,simpleAutoencoder_out3 = createModel(inputs,table,0.01,0.01,False,1,False,0.1,X.shape[1])\n",
    "    models.append(simpleAutoencoder3)\n",
    "    outs.append(simpleAutoencoder_out3)\n",
    "    \n",
    "    #sparse model\n",
    "    simpleAutoencoder2,simpleAutoencoder_out2 = createModel(inputs,table,0,0,True,1,False,0.1,X.shape[1])\n",
    "    models.append(simpleAutoencoder2)\n",
    "    outs.append(simpleAutoencoder_out2)\n",
    "    #sparse dropout model\n",
    "\n",
    "    simpleAutoencoder6,simpleAutoencoder_out6 = createModel(inputs,table,0,0,True,1,True,0.1,X.shape[1])\n",
    "    models.append(simpleAutoencoder6)\n",
    "    outs.append(simpleAutoencoder_out6)\n",
    "    #denoising model\n",
    "\n",
    "    simpleAutoencoder7,simpleAutoencoder_out7 = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "    models.append(simpleAutoencoder7)\n",
    "    outs.append(simpleAutoencoder_out7)\n",
    "    \n",
    "    \n",
    "#     #ion best archs\n",
    "    \n",
    "#     #simple model\n",
    "#     table = [100,70,40,10,40,70,100]\n",
    "#     simpleAutoencoder1,simpleAutoencoder_out1 = createModel(inputs,table,0,0,False,1,False,0.1,X.shape[1])\n",
    "#     models.append(simpleAutoencoder1)\n",
    "#     outs.append(simpleAutoencoder_out1)\n",
    "    \n",
    "#     #contracting model\n",
    "#     table = [200,100,70,40,10,40,70,100,200]\n",
    "\n",
    "#     simpleAutoencoder3,simpleAutoencoder_out3 = createModel(inputs,table,0.01,0.01,False,1,False,0.1,X.shape[1])\n",
    "#     models.append(simpleAutoencoder3)\n",
    "#     outs.append(simpleAutoencoder_out3)\n",
    "    \n",
    "#     #sparse model\n",
    "#     table = [100,70,40,10,40,70,100]\n",
    "    \n",
    "#     simpleAutoencoder2,simpleAutoencoder_out2 = createModel(inputs,table,0,0,True,1,False,0.1,X.shape[1])\n",
    "#     models.append(simpleAutoencoder2)\n",
    "#     outs.append(simpleAutoencoder_out2)\n",
    "#     #sparse dropout model\n",
    "    \n",
    "#     simpleAutoencoder6,simpleAutoencoder_out6 = createModel(inputs,table,0,0,True,1,True,0.1,X.shape[1])\n",
    "#     models.append(simpleAutoencoder6)\n",
    "#     outs.append(simpleAutoencoder_out6)\n",
    "#     #denoising model\n",
    "#     table = [200,100,70,40,10,40,70,100,200]\n",
    "\n",
    "#     simpleAutoencoder7,simpleAutoencoder_out7 = createModel(inputs,table,kernel_g,bias_g,sparse_g,sparsity_g,dropout_g,dropoutRate_g,X.shape[1])\n",
    "#     models.append(simpleAutoencoder7)\n",
    "#     outs.append(simpleAutoencoder_out7)\n",
    "    \n",
    "    \n",
    "#      #sat best archs\n",
    "    \n",
    "#     #simple model\n",
    "#     table = [2000,1000,500,300,200,100,70,40,10,40,70,100,200,300,500,1000,2000]\n",
    "#     simpleAutoencoder1,simpleAutoencoder_out1 = createModel(inputs,table,0,0,False,1,False,0.1,X.shape[1])\n",
    "#     models.append(simpleAutoencoder1)\n",
    "#     outs.append(simpleAutoencoder_out1)\n",
    "    \n",
    "#     #contracting model\n",
    "    \n",
    "\n",
    "#     simpleAutoencoder3,simpleAutoencoder_out3 = createModel(inputs,table,0.01,0.01,False,1,False,0.1,X.shape[1])\n",
    "#     models.append(simpleAutoencoder3)\n",
    "#     outs.append(simpleAutoencoder_out3)\n",
    "    \n",
    "#     #sparse model\n",
    "    \n",
    "    \n",
    "#     simpleAutoencoder2,simpleAutoencoder_out2 = createModel(inputs,table,0,0,True,1,False,0.1,X.shape[1])\n",
    "#     models.append(simpleAutoencoder2)\n",
    "#     outs.append(simpleAutoencoder_out2)\n",
    "#     #sparse dropout model\n",
    "    \n",
    "#     simpleAutoencoder6,simpleAutoencoder_out6 = createModel(inputs,table,0,0,True,1,True,0.1,X.shape[1])\n",
    "#     models.append(simpleAutoencoder6)\n",
    "#     outs.append(simpleAutoencoder_out6)\n",
    "#     #denoising model\n",
    "    \n",
    "\n",
    "#     simpleAutoencoder7,simpleAutoencoder_out7 = createModel(inputs,table,0,0,False,1,False,0.1,X.shape[1])\n",
    "#     models.append(simpleAutoencoder7)\n",
    "#     outs.append(simpleAutoencoder_out7)\n",
    "    \n",
    "    \n",
    "\n",
    "#     for i in range(len(models)):\n",
    "#         if(denoising):\n",
    "#             models[i].fit(X_train_noisy, X_train,\n",
    "#                             epochs=epochs,\n",
    "#                             batch_size=256,\n",
    "#                             shuffle=True,\n",
    "#                             callbacks=callbacks,\n",
    "#                             validation_split=0.4,\n",
    "#                             verbose=0\n",
    "#                            )\n",
    "#             decoded = models[i].predict(X_test)\n",
    "#             mse = np.mean(np.power(X_test - decoded, 2), axis=1)\n",
    "#             mse = mse.reshape(-1, 1)\n",
    "#             scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#             mse = scaler.fit_transform(mse)\n",
    "#             mses.append(mse)\n",
    "#         else:\n",
    "#             models[i].fit(X_train, X_train,\n",
    "#                             epochs=epochs,\n",
    "#                             batch_size=256,\n",
    "#                             shuffle=True,\n",
    "#                             callbacks=callbacks,\n",
    "#                             validation_split=0.4,\n",
    "#                             verbose=0\n",
    "#                            )\n",
    "#             decoded = models[i].predict(X_test)\n",
    "#             mse = np.mean(np.power(X_test - decoded, 2), axis=1)\n",
    "#             mse = mse.reshape(-1, 1)\n",
    "#             scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#             mse = scaler.fit_transform(mse)\n",
    "#             mses.append(mse)\n",
    "            \n",
    "            \n",
    "    for i in range(len(models)):\n",
    "        if(i<(len(models)-1)):\n",
    "            models[i].fit(X_train, X_train,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=256,\n",
    "                            shuffle=True,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_split=0.3,\n",
    "                            verbose=0\n",
    "                           )\n",
    "            \n",
    "            decoded = models[i].predict(X_test)\n",
    "            decodedTrain = models[i].predict(X_train)\n",
    "            mseTrain = np.mean(np.power(X_train - decodedTrain, 2), axis=1)\n",
    "            mseTrain = mseTrain.reshape(-1, 1)\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            #mseTrain = scaler.fit_transform(mseTrain)\n",
    "            msesTrain.append(mseTrain)\n",
    "            \n",
    "            mse = np.mean(np.power(X_test - decoded, 2), axis=1)\n",
    "            #mse = mse.values.reshape(-1, 1)\n",
    "            mse = mse.reshape(-1, 1)\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            #mse = scaler.fit_transform(mse)\n",
    "            mses.append(mse)\n",
    "        else:\n",
    "            models[i].fit(X_train_noisy, X_train,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=256,\n",
    "                            shuffle=True,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_split=0.3,\n",
    "                            verbose=0\n",
    "                           )\n",
    "            \n",
    "            decoded = models[i].predict(X_test)\n",
    "            mse = np.mean(np.power(X_test - decoded, 2), axis=1)\n",
    "            #mse = mse.values.reshape(-1, 1)\n",
    "            mse = mse.reshape(-1, 1)\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            #mse = scaler.fit_transform(mse)\n",
    "            mses.append(mse)\n",
    "            decodedTrain = models[i].predict(X_train_noisy)\n",
    "            mseTrain = np.mean(np.power(X_train_noisy - decodedTrain, 2), axis=1)\n",
    "            mseTrain = mseTrain.reshape(-1, 1)\n",
    "            #mseTrain = scaler.fit_transform(mseTrain)\n",
    "            msesTrain.append(mseTrain)\n",
    "        \n",
    "    \n",
    "    temp = np.array(mses)\n",
    "    ensemble = (np.sum(temp,axis = 0)/(len(mses)))\n",
    "    #print(ensemble)\n",
    "    mses.append(ensemble)\n",
    "    \n",
    "    \n",
    "  \n",
    "    #x = concatenate([decoded3_de2, decoded3_s3,decoded3,decoded3_s2])  # merge the outputs of the two models\n",
    "    x = Average()(outs) \n",
    "\n",
    "    out = Dense(X.shape[1],activation='tanh'\n",
    "                #, kernel_regularizer=l2(kernel_reg) \n",
    "                #, bias_regularizer=l2(bias_reg)\n",
    "               )(x)  # final layer of the network\n",
    "    Emodel = Model(inputs=inputs, outputs=out)\n",
    "    for l in Emodel.layers:\n",
    "        l.trainable = False\n",
    "\n",
    "    Emodel.layers[len(Emodel.layers)-1].trainable = True\n",
    "\n",
    "    Emodel.compile(optimizer='adam', loss='mse')\n",
    "    Emodel.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_split=0.4,\n",
    "                    verbose=0\n",
    "    )\n",
    "\n",
    "    ensemble_pred = Emodel.predict(X_test)\n",
    "    #print(ensemble_pred)\n",
    "    ensemble2_mse = np.mean(np.power(X_test - ensemble_pred, 2), axis=1)\n",
    "    #ensemble2_mse = ensemble2_mse.values.reshape(-1, 1)\n",
    "    ensemble2_mse = ensemble2_mse.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #ensemble2_mse = scaler.fit_transform(ensemble2_mse)\n",
    "    #print(ensemble2_mse)\n",
    "    mses.append(ensemble2_mse)\n",
    "    decodedTrain = Emodel.predict(X_train)\n",
    "    mseTrain = np.mean(np.power(X_train - decodedTrain, 2), axis=1)\n",
    "    mseTrain = mseTrain.reshape(-1, 1)\n",
    "    #mseTrain = scaler.fit_transform(mseTrain)\n",
    "    msesTrain.append(mseTrain)\n",
    "    \n",
    "    for i in range(len(mses)):\n",
    "        \n",
    "        fpr_keras3, tpr_keras3, thresholds_keras3 = roc_curve(y_test, mses[i])\n",
    "        auc_keras3 = auc(fpr_keras3, tpr_keras3)\n",
    "        aucs.append(auc_keras3)\n",
    "        #plt.figure(1)\n",
    "        #plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "        #plt.plot(fpr_keras3, tpr_keras3, label='Keras (area = {:.3f})'.format(auc_keras3))\n",
    "        #print(auc_keras3)\n",
    "    #plt.xlabel('False positive rate')\n",
    "    #plt.ylabel('True positive rate')\n",
    "    #plt.title('ROC curve')\n",
    "    #plt.legend(loc='best')\n",
    "    #plt.show()    \n",
    "    \n",
    "    print('results')\n",
    "    \n",
    "    #3% tha thewrw lathos apo ta swsta -> predict apo ta training dedomena gia na vgei to threshold\n",
    "#     votes = [];\n",
    "#     threshes = [];\n",
    "#     for i in range(len(mses)):\n",
    "#         threshes.append(Find_Optimal_Cutoff(y_test,mses[i]))\n",
    "#     for i in range(len(threshes)):\n",
    "#         temp = mses[i]\n",
    "#         tempvotes = []\n",
    "#         for j in range(len(y_test)):\n",
    "#             if(temp[j]>threshes[i]):\n",
    "#                 tempvotes.append(1)\n",
    "#             else:\n",
    "#                 tempvotes.append(0)\n",
    "#         votes.append(tempvotes)\n",
    "    y_test_list = y_test.values.tolist()\n",
    "\n",
    "#     results_with_thresh = []\n",
    "#     for j in range(len(threshes)):    \n",
    "#         sumss = 0;\n",
    "#         for i in range(len(y_test_list)):\n",
    "#             if(votes[j][i]==y_test_list[i][0]):\n",
    "#                 sumss = sumss+1\n",
    "#         #print(str(vote_result[i])+'\\t '+ str(y_test_list[i]))  \n",
    "#         results_with_thresh.append(sumss/len(y_test_list));\n",
    "#         #print(sumss/len(y_test_list))\n",
    "    \n",
    "#     vote_result = []\n",
    "#     for i in range(len(y_test_list)):\n",
    "#         if((votes[0][i]+votes[1][i]+votes[2][i]+votes[3][i]+votes[4][i])/5>=0.5):\n",
    "#             vote_result.append(1)\n",
    "#         else:\n",
    "#             vote_result.append(0)\n",
    "\n",
    "#     sumss = 0;\n",
    "#     for i in range(len(y_test_list)):\n",
    "#         if(vote_result[i]==y_test_list[i][0]):\n",
    "#             sumss = sumss+1\n",
    "#         #print(str(vote_result[i])+'\\t '+ str(y_test_list[i]))        \n",
    "#     results_with_thresh.append(sumss/len(y_test_list))\n",
    "    #print(\"results_with_thresh\",sumss/len(y_test_list))\n",
    "\n",
    "   \n",
    "           \n",
    "    \n",
    "\n",
    "\n",
    "    maxThres = 0;\n",
    "#     for i in range(len(mses)):\n",
    "#         scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#         mses[i] = scaler.fit_transform(mses[i])\n",
    "#     for i in range(len(msesTrain)):\n",
    "#         scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#         msesTrain[i] = scaler.fit_transform(msesTrain[i])\n",
    "\n",
    "    threshez = []\n",
    "    for i in range(len(msesTrain)):\n",
    "        threshez.append(-np.sort(-msesTrain[i],axis =0)[math.ceil((msesTrain[0].shape[0])*percentChosen)][0])    \n",
    "    \n",
    "#     votes = [];\n",
    "#     threshes = [];\n",
    "\n",
    "#     for i in range(len(mses)):\n",
    "#         threshes.append(Find_Optimal_Cutoff(y_test,mses[i]))\n",
    "#     for i in range(len(threshes)):\n",
    "#         temp = mses[i]\n",
    "#         tempvotes = []\n",
    "#         for j in range(len(y_test)):\n",
    "#             if(temp[j]>threshes[i]):\n",
    "#                 tempvotes.append(1)\n",
    "#             else:\n",
    "#                 tempvotes.append(0)\n",
    "#         votes.append(tempvotes)\n",
    "#     votes = np.array(votes)\n",
    "\n",
    "#     performanceSingle = []\n",
    "#     y_test_list = y_test.values.tolist()\n",
    "#     for j in range(len(threshes)):    \n",
    "#         sumss = 0;\n",
    "#         for i in range(len(y_test_list)):\n",
    "#             if(votes[j][i]==y_test_list[i][0]):\n",
    "#                 sumss = sumss+1\n",
    "#         performanceSingle.append(sumss/len(y_test_list))\n",
    "    \n",
    "    \n",
    "    votes = [];\n",
    "    #votes_negative = [];\n",
    "    \n",
    "    for i in range(len(threshez)):\n",
    "        temp = mses[i]\n",
    "        tempvotes = []\n",
    "        for j in range(len(y_test)):\n",
    "            if(mses[i][j]>threshez[i] ):\n",
    "                tempvotes.append(1)\n",
    "            else:\n",
    "                tempvotes.append(0)\n",
    "        votes.append(tempvotes)\n",
    "    votes = np.array(votes)\n",
    "\n",
    "    performanceIndividual = []\n",
    "    for j in range(len(threshez)):    \n",
    "        sumss = 0;\n",
    "        for i in range(len(y_test_list)):\n",
    "            if(votes[j][i]==y_test_list[i][0]):\n",
    "                sumss = sumss+1\n",
    "        performanceIndividual.append(sumss/len(y_test_list))\n",
    "    \n",
    "    \n",
    "    print(\"performanceIndividual\",performanceIndividual)\n",
    "         \n",
    "    distances_from_thresh = []\n",
    "    for i in range(len(threshez)):\n",
    "        temp = mses[i]\n",
    "        tempdistances = []\n",
    "        for j in range(len(msesTrain[0])):\n",
    "            tempdistances.append(msesTrain[i][j][0]-threshez[i])\n",
    "        distances_from_thresh.append(tempdistances)\n",
    "    #print(distances_from_thresh)\n",
    "    distances_from_thresh = np.array(distances_from_thresh)\n",
    "    distances_from_thresh = np.tanh(distances_from_thresh)\n",
    "#     for i in range(len(distances_from_thresh)):\n",
    "#         distances_from_thresh[i] = (distances_from_thresh[i] - np.min(distances_from_thresh[i])/np.ptp(distances_from_thresh[i]))\n",
    "\n",
    "    distance_from_thresh_0 = distances_from_thresh[0].mean()\n",
    "    #print(distances_from_thresh[1].mean())\n",
    "    distance_from_thresh_1 = distances_from_thresh[1].mean()\n",
    "    #print(distances_from_thresh[2].mean())\n",
    "    distance_from_thresh_2 = distances_from_thresh[2].mean()\n",
    "    #print(distances_from_thresh[3].mean())\n",
    "    distance_from_thresh_3 = distances_from_thresh[3].mean()\n",
    "    #print(distances_from_thresh[4].mean())\n",
    "    distance_from_thresh_4 = distances_from_thresh[4].mean()\n",
    "    #print(distances_from_thresh[5].mean())\n",
    "    distance_from_thresh_5 = distances_from_thresh[5].mean()\n",
    "    #print(\"values\")\n",
    "    mean_of_means = np.mean([distances_from_thresh[0].mean(),distances_from_thresh[1].mean(),distances_from_thresh[2].mean(),distances_from_thresh[3].mean(),distances_from_thresh[4].mean(),distances_from_thresh[5].mean()])\n",
    "    #print(\"mean_of_means\")\n",
    "\n",
    "\n",
    "    ttt = []\n",
    "    for i in range(len(distances_from_thresh[0])):\n",
    "        ttt.append((distances_from_thresh[0][i]+distances_from_thresh[1][i]+distances_from_thresh[2][i]+distances_from_thresh[3][i]+distances_from_thresh[4][i]+distances_from_thresh[5][i])/6)\n",
    "    #plt.plot(distances_from_thresh[1],color=\"blue\")\n",
    "    #plt.plot(distances_from_thresh[1],color=\"red\")\n",
    "    #plt.plot(distances_from_thresh[0])\n",
    "    #plt.plot(distances_from_thresh[2])\n",
    "\n",
    "    #plt.plot(distances_from_thresh[3])\n",
    "    #plt.plot(distances_from_thresh[4])\n",
    "    #plt.plot(distances_from_thresh[5])\n",
    "\n",
    "    distances_from_thresh = []\n",
    "    for i in range(len(threshez)):\n",
    "        temp = mses[i]\n",
    "        tempdistances = []\n",
    "        for j in range(len(y_test)):\n",
    "            tempdistances.append(mses[i][j][0]-threshez[i])\n",
    "        distances_from_thresh.append(tempdistances)\n",
    "    #print(distances_from_thresh)\n",
    "    distances_from_thresh = np.array(distances_from_thresh)\n",
    "    distances_from_thresh = np.tanh(distances_from_thresh)\n",
    "#     for i in range(len(distances_from_thresh)):\n",
    "#         distances_from_thresh[i] = (distances_from_thresh[i] - np.min(distances_from_thresh[i])/np.ptp(distances_from_thresh[i]))\n",
    "\n",
    "    print(distances_from_thresh[0].mean())\n",
    "    print(distances_from_thresh[1].mean())\n",
    "    print(distances_from_thresh[2].mean())\n",
    "    print(distances_from_thresh[3].mean())\n",
    "    print(distances_from_thresh[4].mean())\n",
    "    print(distances_from_thresh[5].mean())\n",
    "    print(\"values\")\n",
    "    # print(distances_from_thresh[0][20])\n",
    "    # print(distances_from_thresh[1][20])\n",
    "    # print(distances_from_thresh[2][20])\n",
    "    # print(distances_from_thresh[3][20])\n",
    "    # print(distances_from_thresh[4][20])\n",
    "    # print(distances_from_thresh[5][20])\n",
    "\n",
    "    ttt = []\n",
    "    for i in range(len(distances_from_thresh[0])):\n",
    "        ttt.append((distances_from_thresh[0][i]+distances_from_thresh[1][i]+distances_from_thresh[2][i]+distances_from_thresh[3][i]+distances_from_thresh[4][i]+distances_from_thresh[5][i])/6)\n",
    "\n",
    "    distances_from_thres=[]\n",
    "    distances_from_thres.append(distance_from_thresh_0)\n",
    "    distances_from_thres.append(distance_from_thresh_1)\n",
    "    distances_from_thres.append(distance_from_thresh_2)\n",
    "    distances_from_thres.append(distance_from_thresh_3)\n",
    "    distances_from_thres.append(distance_from_thresh_4)\n",
    "    distances_from_thres.append(distance_from_thresh_5)\n",
    "#     ps = []\n",
    "#     for i in range(len(threshez)):\n",
    "\n",
    "#         tempP=[]\n",
    "#         for j in range(len(y_test)):\n",
    "\n",
    "#             if(mses[i][j]>=threshez[i] ):\n",
    "#                 tempP.append(0.5+(0.5*(mses[i][j]-threshez[i])/(threshez[i]-distances_from_thres[i])))\n",
    "#             else:\n",
    "#                 tempP.append((0.5*(mses[i][j]-distances_from_thres[i])/(threshez[i]-distances_from_thres[i])))\n",
    "\n",
    "#         ps.append(tempP)     \n",
    "#     plt.axhline(y=0.5, color='r', linestyle='-')\n",
    "    ps=[]\n",
    "    ps2=[]\n",
    "    for i in range(len(threshez)):\n",
    "        tempP=[]\n",
    "        tempP2=[]\n",
    "        for j in range(len(y_test)):\n",
    "            if(mses[i][j]>=(threshez[i]) ):\n",
    "                tempP2.append(0.5+((0.5)*((mses[i][j]-threshez[i]))/(threshez[i]-math.sqrt(distances_from_thres[i]** 2))))\n",
    "                if(distances_from_thres[i]<0):\n",
    "                    tempP.append(0.5+abs(distances_from_thres[i])+((0.5)*((mses[i][j]-threshez[i]))/(threshez[i]-(distances_from_thres[i]))))\n",
    "                else:\n",
    "                    tempP.append(0.5+((0.5)*((mses[i][j]-threshez[i]))/(threshez[i]-(distances_from_thres[i]))))\n",
    "\n",
    "            else:\n",
    "                tempP2.append(((0.5)*((mses[i][j]-math.sqrt(distances_from_thres[i]**2)))/(threshez[i]-math.sqrt(distances_from_thres[i]**2))))\n",
    "                if(distances_from_thres[i]<0):\n",
    "                    tempP.append(((0.5)*((mses[i][j]-distances_from_thres[i]))/(threshez[i]-(distances_from_thres[i]))))\n",
    "                else:\n",
    "                    tempP.append(((0.5)*((mses[i][j]-distances_from_thres[i]))/(threshez[i]-(distances_from_thres[i]))))\n",
    "\n",
    "        ps.append(tempP)     \n",
    "        ps2.append(tempP2)     \n",
    "#     for i in range(len(ps)):\n",
    "#         ps[i] = (ps[i] - np.min(ps[i])/np.ptp(ps[i]))\n",
    "    \n",
    "    vote_result = []\n",
    "    #vote_result_negative = []\n",
    "    vote_result_distance = []\n",
    "    vote_result_distance_temp = []\n",
    "    vote_result_distance_temp2 = []\n",
    "    vote_result_distance_p = []\n",
    "    vote_result_distance_p2 = []\n",
    "    for i in range(len(y_test_list)):\n",
    "        if((0\n",
    "            +1*votes[0][i]\n",
    "            +1*votes[1][i]\n",
    "            +1*votes[2][i]\n",
    "            +1*votes[3][i]\n",
    "            +1*votes[4][i]\n",
    "            +1*votes[5][i]\n",
    "            #+votes[6][i]\n",
    "            )/6>=0.5):\n",
    "            vote_result.append(1)\n",
    "        else:\n",
    "            vote_result.append(0)\n",
    "        #temp = votes_negative[0][i]+votes_negative[1][i]+votes_negative[2][i]+votes_negative[3][i]+votes_negative[4][i]+votes_negative[5][i]\n",
    "\n",
    "        temp_distance2 = 0;\n",
    "        if(distances_from_thresh[0][i]>distance_from_thresh_0):\n",
    "            temp_distance2 = temp_distance2 +1;\n",
    "        else:\n",
    "            temp_distance2 = temp_distance2 -1;\n",
    "\n",
    "        if(distances_from_thresh[1][i]>distance_from_thresh_1):\n",
    "            temp_distance2 = temp_distance2 +1;\n",
    "        else:\n",
    "            temp_distance2 = temp_distance2 -1;\n",
    "\n",
    "        if(distances_from_thresh[2][i]>distance_from_thresh_2):\n",
    "            temp_distance2 = temp_distance2 +1;\n",
    "        else:\n",
    "            temp_distance2 = temp_distance2 -1;\n",
    "        if(distances_from_thresh[3][i]>distance_from_thresh_3):\n",
    "            temp_distance2 = temp_distance2 +1;\n",
    "        else:\n",
    "            temp_distance2 = temp_distance2 -1;\n",
    "        if(distances_from_thresh[4][i]>distance_from_thresh_4):\n",
    "            temp_distance2 = temp_distance2 +1;\n",
    "        else:\n",
    "            temp_distance2 = temp_distance2 -1;\n",
    "        if(distances_from_thresh[5][i]>distance_from_thresh_5):\n",
    "            temp_distance2 = temp_distance2 +1;\n",
    "        else:\n",
    "            temp_distance2 = temp_distance2 -1;\n",
    "\n",
    "        temp_distance =  (distances_from_thresh[0][i]+distances_from_thresh[1][i]+distances_from_thresh[2][i]+distances_from_thresh[3][i]+distances_from_thresh[4][i]+distances_from_thresh[5][i])/6\n",
    "        temp_distance_p =  (ps[0][i]+ps[1][i]+ps[2][i]+ps[3][i]+ps[4][i]+ps[5][i])/6\n",
    "        temp_distance_p2 =  (ps2[0][i]+ps2[1][i]+ps2[2][i]+ps2[3][i]+ps2[4][i]+ps2[5][i])/6\n",
    "\n",
    "        if(temp_distance>mean_of_means):\n",
    "            vote_result_distance.append(1)\n",
    "        else:\n",
    "            vote_result_distance.append(0)\n",
    "\n",
    "        if(temp_distance2>=0):\n",
    "            vote_result_distance_temp2.append(1)\n",
    "        else:\n",
    "            vote_result_distance_temp2.append(0)\n",
    "        if(temp_distance_p>=0.5):\n",
    "            vote_result_distance_p.append(1)\n",
    "        else:\n",
    "            vote_result_distance_p.append(0)\n",
    "        if(temp_distance_p2>=0.5):\n",
    "            vote_result_distance_p2.append(1)\n",
    "        else:\n",
    "            vote_result_distance_p2.append(0)\n",
    "\n",
    "    performanceSum = 0;\n",
    "    #performanceSumNegative = 0;\n",
    "    performanceSumDistance = 0;\n",
    "    performanceSumDistance2 = 0;\n",
    "    performanceSumDistanceP = 0;\n",
    "    performanceSumDistanceP2 = 0;\n",
    "    for j in range(len(vote_result_distance)):\n",
    "        if(vote_result_distance[j] == y_test_list[j][0]):\n",
    "            performanceSumDistance = performanceSumDistance + 1;\n",
    "        if(vote_result_distance_temp2[j] == y_test_list[j][0]):\n",
    "            performanceSumDistance2 = performanceSumDistance2 + 1;    \n",
    "        if(vote_result_distance_p[j] == y_test_list[j][0]):\n",
    "            performanceSumDistanceP = performanceSumDistanceP + 1;    \n",
    "        if(vote_result_distance_p2[j] == y_test_list[j][0]):\n",
    "            performanceSumDistanceP2 = performanceSumDistanceP2 + 1;    \n",
    "            \n",
    "    #print(performanceIndividual)\n",
    "    print(\"network vote_result\")\n",
    "    print(\"performance \",max(performanceIndividual));\n",
    "    #print(\"performance Negative \",performanceSumNegative/len(y_test)); \n",
    "    print(\"performance Distance \",performanceSumDistance/len(y_test));\n",
    "    print(\"performance Distance positive negative votes \",performanceSumDistance2/len(y_test));\n",
    "    print(\"performance DistanceP \",performanceSumDistanceP/len(y_test));\n",
    "    print(\"performance DistanceP2 \",performanceSumDistanceP2/len(y_test));\n",
    "    #performanceSum = performanceIndividual/len(y_test)\n",
    "    #performanceSumNegative = performanceSumNegative/len(y_test)\n",
    "    performanceSumDistance = performanceSumDistance/len(y_test)\n",
    "    performanceSumDistance2 = performanceSumDistance2/len(y_test)\n",
    "    performanceSumDistanceP = performanceSumDistanceP/len(y_test)\n",
    "    performanceSumDistanceP2 = performanceSumDistanceP2/len(y_test)\n",
    "    print(\"\")\n",
    "    \n",
    "    if(k==0):\n",
    "        sums = aucs;\n",
    "        perf = max(performanceIndividual)\n",
    "        #sums_votes = results_with_thresh\n",
    "        #sums_performanceSingle = performanceSingle\n",
    "        sums_performanceIndividual = performanceIndividual\n",
    "        #sums_performanceSum = performanceSum\n",
    "        sums_performanceSumDistance = performanceSumDistance\n",
    "        sums_performanceSumDistance2 = performanceSumDistance2\n",
    "        sums_performanceSumDistanceP = performanceSumDistanceP\n",
    "        sums_performanceSumDistanceP2 = performanceSumDistanceP2\n",
    "    else:\n",
    "        temp = aucs\n",
    "        #temp2 = results_with_thresh\n",
    "        #temp3 = performanceSingle\n",
    "        perfTemp  = max(performanceIndividual)\n",
    "        temp4 = performanceIndividual\n",
    "        #temp5 = performanceSum\n",
    "        temp7 = performanceSumDistance\n",
    "        temp8 = performanceSumDistance2\n",
    "        for tempi in range(0,len(sums)):\n",
    "            sums[tempi] = sums[tempi]+temp[tempi];\n",
    "        #for tempi2 in range(0,len(results_with_thresh)):\n",
    "            #sums_votes[tempi2] = sums_votes[tempi2]+temp2[tempi2]\n",
    "#         for tempi3 in range(0,len(performanceSingle)):\n",
    "#             sums_performanceSingle[tempi3] = sums_performanceSingle[tempi3]+temp3[tempi3]    \n",
    "        for tempi4 in range(0,len(performanceIndividual)):\n",
    "            sums_performanceIndividual[tempi4] = sums_performanceIndividual[tempi4]+temp4[tempi4]    \n",
    "        #sums_performanceSum = sums_performanceSum + performanceSum\n",
    "        perf = perf + perfTemp\n",
    "        sums_performanceSumDistance = sums_performanceSumDistance+performanceSumDistance\n",
    "        sums_performanceSumDistance2 = sums_performanceSumDistance2+performanceSumDistance2\n",
    "        sums_performanceSumDistanceP = sums_performanceSumDistanceP+performanceSumDistanceP\n",
    "        sums_performanceSumDistanceP2 = sums_performanceSumDistanceP2+performanceSumDistanceP2\n",
    "\n",
    "    import csv\n",
    "    from datetime import date\n",
    "    with open('resultsGeneralPerformance.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if(k==iterations-1):\n",
    "            today = date.today()\n",
    "            d1 = today.strftime(\"%d/%m/%Y\")\n",
    "            writer.writerow([d1])\n",
    "            writer.writerow([file_name])\n",
    "            writer.writerow([message])\n",
    "            writer.writerow([\"params\",\"noise_amount\",noise_amount,\"iterations\" ,iterations])\n",
    "#             writer.writerow([\"Simple\",\"Contracting\",\"Sparse\",\"Sparse Dropout\",\"Denoising\",\"Ensemble\",\"Ensemble2\"])\n",
    "#             #writer.writerow([\"Model 8L\",\"Model 7L\",\"Model 6L\",\"Model 5L\",\"Model 4L\",\"Model 3L\",\"Model 2L\",\"Model 1L\",\"Ensemble\",\"Ensemble2\"])\n",
    "#             writer.writerow([x/iterations for x in sums])\n",
    "#             writer.writerow(['thresholds']);\n",
    "#             writer.writerow(threshes);\n",
    "#             writer.writerow(['results']);\n",
    "#             writer.writerow([\"Simple\",\"Contracting\",\"Sparse\",\"Sparse Dropout\",\"Denoising\",\"Ensemble\",\"Ensemble2\",\"Voting\"])\n",
    "#             writer.writerow([x/iterations for x in sums_votes]);\n",
    "            #voting\n",
    "            writer.writerow([\"network size\" ]);\n",
    "            writer.writerow(table);\n",
    "            writer.writerow([\"Simple\",\"Contracting\",\"Sparse\",\"Sparse Dropout\",\"Denoising\",\"Ensemble2\",\"Voting\"])\n",
    "            #writer.writerow([x/iterations for x in sums_performanceSingle]);\n",
    "            writer.writerow([x/iterations for x in sums_performanceIndividual]);\n",
    "            #writer.writerow([sums_performanceSum/iterations ]);\n",
    "            writer.writerow([\"performance max single\",str(perf/iterations) ]);\n",
    "            writer.writerow([\"performance Distance\",str(sums_performanceSumDistance/iterations)]);\n",
    "            writer.writerow([\"performance Distance positive negative votes\",str(sums_performanceSumDistance2/iterations)] );\n",
    "            writer.writerow([\"performance DistanceP\",str(sums_performanceSumDistanceP/iterations)] );\n",
    "            writer.writerow([\"performance DistanceP2\",str(sums_performanceSumDistanceP2/iterations)] );\n",
    "            print(\"performance max single\",str(perf/iterations))\n",
    "            print(\"performance Distance\",str(sums_performanceSumDistance/iterations)) \n",
    "            print(\"performance Distance positive negative votes\",str(sums_performanceSumDistance2/iterations))\n",
    "            print(\"performance DistanceP\",str(sums_performanceSumDistanceP/iterations))\n",
    "            print(\"performance DistanceP2\",str(sums_performanceSumDistanceP2/iterations))\n",
    "            \n",
    "    print(k)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "maxThres = 0;\n",
    "\n",
    "# for i in range(len(mses)):\n",
    "#     #print()\n",
    "# #     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# #     mses[i] = scaler.fit_transform(mses[i])\n",
    "#     mses[i] = sigmoid(mses[i])\n",
    "# for i in range(len(msesTrain)):\n",
    "#     print()\n",
    "#     #scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     #msesTrain[i] = scaler.fit_transform(msesTrain[i])\n",
    "#     msesTrain[i] = sigmoid(msesTrain[i])\n",
    "\n",
    "\n",
    "plt.plot(mses[0][0:50])\n",
    "#print(msesTrain[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(msesTrain[0][0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mses[2][0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(msesTrain[2][0:50],color=\"green\")\n",
    "plt.plot(y_test[0:50],color=\"red\")\n",
    "plt.plot(mses[2][0:50],color=\"blue\")\n",
    "threshez = []\n",
    "for i in range(len(msesTrain)):\n",
    "    #print(msesTrain[i][msesTrain[i]>0.8])\n",
    "    #print((msesTrain[0].shape[0])*0.02)\n",
    "    #print(msesTrain[1].shape)\n",
    "    print(-np.sort(-msesTrain[i],axis =0)[math.ceil((msesTrain[0].shape[0])*percentChosen)])\n",
    "    threshez.append(-np.sort(-msesTrain[i],axis =0)[math.ceil((msesTrain[0].shape[0])*percentChosen)][0])\n",
    "#print(threshez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.axhline(y=msesTrain[2],color=\"green\")\n",
    "import matplotlib.transforms as transforms\n",
    "from matplotlib.lines import Line2D\n",
    "fig= plt.figure()\n",
    "fig2, ax = plt.subplots()\n",
    "custom_lines = [Line2D([0], [0], color='r', lw=4)]\n",
    "ax.legend(custom_lines, ['Threshold'])\n",
    "fig2.suptitle('Mean Squared Errors', fontsize=20)\n",
    "\n",
    "# plt.xlabel('xlabel', fontsize=18)\n",
    "# plt.ylabel('ylabel', fontsize=16)\n",
    "plt.axhline(y=(threshez[2]), color='r', linestyle='-')\n",
    "plt.plot(msesTrain[2],color=\"blue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mses)):\n",
    "        \n",
    "        fpr_keras3, tpr_keras3, thresholds_keras3 = roc_curve(y_test, mses[i])\n",
    "        auc_keras3 = auc(fpr_keras3, tpr_keras3)\n",
    "        #aucs.append(auc_keras3)\n",
    "        plt.figure(1)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "        plt.plot(fpr_keras3, tpr_keras3, label='Keras (area = {:.3f})'.format(auc_keras3))\n",
    "        print(auc_keras3)\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "votes = [];\n",
    "threshes = [];\n",
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "#print(y_test)\n",
    "for i in range(len(mses)):\n",
    "    threshes.append(Find_Optimal_Cutoff(y_test,mses[i]))\n",
    "#print(mses[0])\n",
    "for i in range(len(threshes)):\n",
    "    temp = mses[i]\n",
    "    tempvotes = []\n",
    "    for j in range(len(y_test)):\n",
    "        if(temp[j]>threshes[i]):\n",
    "            tempvotes.append(1)\n",
    "        else:\n",
    "            tempvotes.append(0)\n",
    "    votes.append(tempvotes)\n",
    "votes = np.array(votes)\n",
    "#print([pd.DataFrame(votes[0]),y_test])\n",
    "#print(pd.concat([pd.DataFrame(votes[0]), y_test], axis=1))\n",
    "\n",
    "\n",
    "y_test_list = y_test.values.tolist()\n",
    "for j in range(len(threshes)):    \n",
    "    sumss = 0;\n",
    "    for i in range(len(y_test_list)):\n",
    "        if(votes[j][i]==y_test_list[i][0]):\n",
    "            sumss = sumss+1\n",
    "        #print(str(vote_result[i])+'\\t '+ str(y_test_list[i]))  \n",
    "    print(j)\n",
    "    print(sumss/len(y_test_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pare tin apostasi apo to thresh ws determining factor :)\n",
    "\n",
    "votes = [];\n",
    "votes_negative = [];\n",
    "\n",
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "#print(y_test)\n",
    "#print(mses[0])\n",
    "for i in range(len(threshez)):\n",
    "    temp = mses[i]\n",
    "    tempvotes = []\n",
    "    tempvotes_negative = [];\n",
    "    #print(\"thresh\",threshez[i])\n",
    "    for j in range(len(y_test)):\n",
    "        #new thresh\n",
    "#       if(temp[j]-threshez[i]>0):\n",
    "#             if(abs(temp[j]-threshez[i]>0.001)):\n",
    "#                 tempvotes.append(1)\n",
    "#                 tempvotes_negative.append(1)\n",
    "#             else:\n",
    "#                 tempvotes.append(0)\n",
    "#                 tempvotes_negative.append(-1)\n",
    "        #old thresh\n",
    "        #print(mses[i][j])\n",
    "        if(mses[i][j]>threshez[i] ):\n",
    "            tempvotes.append(1)\n",
    "            tempvotes_negative.append(1)\n",
    "#         else:\n",
    "#                 tempvotes.append(0)\n",
    "        else:\n",
    "            tempvotes.append(0)\n",
    "            tempvotes_negative.append(-1)\n",
    "    votes.append(tempvotes)\n",
    "    votes_negative.append(tempvotes_negative)\n",
    "votes = np.array(votes)\n",
    "votes_negative = np.array(votes_negative)\n",
    "#print([pd.DataFrame(votes[0]),y_test])\n",
    "#print(pd.concat([pd.DataFrame(votes[0]), y_test], axis=1))\n",
    "\n",
    "\n",
    "#y_test_list = y_test.values.tolist()\n",
    "for j in range(len(threshez)):    \n",
    "    sumss = 0;\n",
    "    for i in range(len(y_test_list)):\n",
    "        if(votes[j][i]==y_test_list[i][0]):\n",
    "            sumss = sumss+1\n",
    "    print(j)\n",
    "    print(sumss/len(y_test_list))\n",
    "    \n",
    "    \n",
    "distances_from_thresh = []\n",
    "for i in range(len(threshez)):\n",
    "    temp = mses[i]\n",
    "    tempdistances = []\n",
    "    for j in range(len(y_test_list)):\n",
    "        tempdistances.append(mses[i][j][0]-threshez[i])\n",
    "        #print(mses[i][j][0]-threshez[i])\n",
    "    distances_from_thresh.append(tempdistances)\n",
    "#print(distances_from_thresh)\n",
    "distances_from_thresh = np.array(distances_from_thresh)\n",
    "#distances_from_thresh = sigmoid(distances_from_thresh)\n",
    "# for i in range(len(distances_from_thresh)):\n",
    "#     distances_from_thresh[i] = (distances_from_thresh[i] - np.min(distances_from_thresh[i])/np.ptp(distances_from_thresh[i]))\n",
    "\n",
    "# distances_from_thresh[0] = distances_from_thresh[0] - distances_from_thresh[0].mean()\n",
    "# distances_from_thresh[1] = distances_from_thresh[1] - distances_from_thresh[1].mean()\n",
    "# distances_from_thresh[2] = distances_from_thresh[2] - distances_from_thresh[2].mean()\n",
    "# distances_from_thresh[3] = distances_from_thresh[3] - distances_from_thresh[3].mean()\n",
    "# distances_from_thresh[4] = distances_from_thresh[4] - distances_from_thresh[4].mean()\n",
    "# distances_from_thresh[5] = distances_from_thresh[5] - distances_from_thresh[5].mean()\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#distances_from_thresh = scaler.fit_transform(np.array(distances_from_thresh))\n",
    "#print(distances_from_thresh[0])\n",
    "#print(distances_from_thresh[0])\n",
    "#plt.plot(distances_from_thresh[0][0:100])\n",
    "#plt.plot(y_test[0:20])\n",
    "print(distances_from_thresh[0].mean())\n",
    "distance_from_thresh_0 = distances_from_thresh[0].mean()\n",
    "print(distances_from_thresh[1].mean())\n",
    "distance_from_thresh_1 = distances_from_thresh[1].mean()\n",
    "print(distances_from_thresh[2].mean())\n",
    "distance_from_thresh_2 = distances_from_thresh[2].mean()\n",
    "print(distances_from_thresh[3].mean())\n",
    "distance_from_thresh_3 = distances_from_thresh[3].mean()\n",
    "print(distances_from_thresh[4].mean())\n",
    "distance_from_thresh_4 = distances_from_thresh[4].mean()\n",
    "print(distances_from_thresh[5].mean())\n",
    "distance_from_thresh_5 = distances_from_thresh[5].mean()\n",
    "print(\"values\")\n",
    "mean_of_means = np.mean([distances_from_thresh[0].mean(),distances_from_thresh[1].mean(),distances_from_thresh[2].mean(),distances_from_thresh[3].mean(),distances_from_thresh[4].mean(),distances_from_thresh[5].mean()])\n",
    "print(\"mean_of_means\")\n",
    "print(mean_of_means)\n",
    "\n",
    "# print(distances_from_thresh[0][20])\n",
    "# print(distances_from_thresh[1][20])\n",
    "# print(distances_from_thresh[2][20])\n",
    "# print(distances_from_thresh[3][20])\n",
    "# print(distances_from_thresh[4][20])\n",
    "# print(distances_from_thresh[5][20])\n",
    "\n",
    "\n",
    "#plt.plot(distances_from_thresh[1],color=\"red\")\n",
    "#plt.plot(distances_from_thresh[0])\n",
    "plt.axhline(y=(distance_from_thresh_0), color='b', linestyle='-')\n",
    "plt.plot(distances_from_thresh[0][0:50])\n",
    "pre = distances_from_thresh[2]\n",
    "#plt.plot(distances_from_thresh[3])\n",
    "#plt.plot(distances_from_thresh[4])\n",
    "#plt.plot(distances_from_thresh[5])\n",
    "\n",
    "#distances_from_thresh = sigmoid(distances_from_thresh)\n",
    "#distances_from_thresh = np.tanh(distances_from_thresh)\n",
    "# for i in range(len(distances_from_thresh)):\n",
    "#     distances_from_thresh[i] = (distances_from_thresh[i] - np.min(distances_from_thresh[i])/np.ptp(distances_from_thresh[i]))\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#distances_from_thresh = scaler.fit_transform(np.array(distances_from_thresh))\n",
    "#print(distances_from_thresh[0])\n",
    "#print(distances_from_thresh[0])\n",
    "#plt.plot(distances_from_thresh[0][0:100])\n",
    "#plt.plot(y_test[0:20])\n",
    "print(distances_from_thresh[0].mean())\n",
    "print(distances_from_thresh[1].mean())\n",
    "print(distances_from_thresh[2].mean())\n",
    "print(distances_from_thresh[3].mean())\n",
    "print(distances_from_thresh[4].mean())\n",
    "print(distances_from_thresh[5].mean())\n",
    "\n",
    "print(\"values\")\n",
    "# print(distances_from_thresh[0][20])\n",
    "# print(distances_from_thresh[1][20])\n",
    "# print(distances_from_thresh[2][20])\n",
    "# print(distances_from_thresh[3][20])\n",
    "# print(distances_from_thresh[4][20])\n",
    "# print(distances_from_thresh[5][20])\n",
    "#print(max(distances_from_thresh[5]))\n",
    "#plt.plot(distances_from_thresh[0])\n",
    "#plt.plot(distances_from_thresh[2])\n",
    "\n",
    "#plt.plot(distances_from_thresh[3])\n",
    "#plt.plot(distances_from_thresh[4])\n",
    "#plt.plot(distances_from_thresh[5])\n",
    "#print(distances_from_thresh[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axhline(y=(distances_from_thresh[0].mean()), color='b', linestyle='-')\n",
    "plt.plot((distances_from_thresh[0][0:50]))\n",
    "plt.plot(y_test[0:50],color = \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pithanotites\n",
    "distances_from_thres=[]\n",
    "\n",
    "\n",
    "distances_from_thres.append(distance_from_thresh_0)\n",
    "distances_from_thres.append(distance_from_thresh_1)\n",
    "distances_from_thres.append(distance_from_thresh_2)\n",
    "distances_from_thres.append(distance_from_thresh_3)\n",
    "distances_from_thres.append(distance_from_thresh_4)\n",
    "distances_from_thres.append(distance_from_thresh_5)\n",
    "\n",
    "# distances_from_thres.append(sigmoid(distance_from_thresh_0))\n",
    "# distances_from_thres.append(sigmoid(distance_from_thresh_1))\n",
    "# distances_from_thres.append(sigmoid(distance_from_thresh_2))\n",
    "# distances_from_thres.append(sigmoid(distance_from_thresh_3))\n",
    "# distances_from_thres.append(sigmoid(distance_from_thresh_4))\n",
    "# distances_from_thres.append(sigmoid(distance_from_thresh_5))\n",
    "\n",
    "ps = []\n",
    "test = [];\n",
    "test2 = [];\n",
    "for i in range(len(threshez)):\n",
    "    \n",
    "    tempP=[]\n",
    "    for j in range(len(y_test)):\n",
    "        if(i==0):\n",
    "            test.append(((mses[i][j]-threshez[i]))/(threshez[i]-math.sqrt(distances_from_thres[i]**2)))\n",
    "        if(mses[i][j]>=(threshez[i]) ):\n",
    "            if(i==0):\n",
    "                test2.append(1)\n",
    "            \n",
    "            #tempP.append(abs(distances_from_thresh[i].mean())+0.5+(0.5*((mses[i][j]-threshez[i]))/(threshez[i]-(distances_from_thres[i]))))\n",
    "            #print(\"all values over thresh\")\n",
    "            #print(\"mse\",mses[i][j])\n",
    "            #print(\"thres\",threshez[i])\n",
    "            #print(\"mean\",distances_from_thres[i])\n",
    "            if(distances_from_thres[i]<0):\n",
    "                tempP.append(0.5\n",
    "                             #+abs(math.sqrt()distances_from_thres[i] ** 2)\n",
    "                             +((0.5)*((mses[i][j]-threshez[i]))/(threshez[i]-math.sqrt(distances_from_thres[i]** 2))))\n",
    "            else:\n",
    "                tempP.append(0.5+((0.5)*((mses[i][j]-threshez[i]))/(threshez[i]-math.sqrt(distances_from_thres[i]** 2))))\n",
    "\n",
    "        else:\n",
    "            if(i==0):\n",
    "                test2.append(0)\n",
    "            #print(\"all values below thresh\")\n",
    "            #print(\"mse\",mses[i][j])\n",
    "            #print(\"thres\",threshez[i])\n",
    "            #print(\"mean\",distances_from_thres[i])\n",
    "            if(distances_from_thres[i]<0):\n",
    "                tempP.append(((0.5)*((mses[i][j]-math.sqrt(distances_from_thres[i]**2)))/(threshez[i]-math.sqrt(distances_from_thres[i]**2))))\n",
    "            else:\n",
    "                tempP.append(((0.5)*((mses[i][j]-math.sqrt(distances_from_thres[i]**2)))/(threshez[i]-math.sqrt(distances_from_thres[i]**2))))\n",
    "            \n",
    "    ps.append(tempP)     \n",
    "plt.axhline(y=0.5, color='r', linestyle='-')\n",
    "plt.axhline(y=sigmoid(distance_from_thresh_0), color='b', linestyle='-')\n",
    "plt.axhline(y=(distance_from_thresh_0), color='b', linestyle='-')\n",
    "print(threshez[0]-distances_from_thres[0].mean())\n",
    "# for i in range(len(ps)):\n",
    "#     ps[i] = (ps[i] - np.min(ps[i])/np.ptp(ps[i]))\n",
    "\n",
    "plt.plot(ps[0][0:50])\n",
    "plt.plot(y_test[0:50])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.axhline(y=distance_from_thresh_1, color='r', linestyle='-')\n",
    "plt.plot(distances_from_thresh[1][0:50])\n",
    "plt.plot(y_test[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(threshez)\n",
    "plt.bar(range(len(threshez)),threshez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(ps[3]).mean())\n",
    "#print(ps[3].shape)\n",
    "\n",
    "#print(np.array(ps[3]))\n",
    "sss = np.array(ps[3])\n",
    "# for i in range(len(ps[3])):\n",
    "#     sss[i] = sigmoid(sss[i][0])\n",
    "# plt.plot(sss)\n",
    "plt.plot(y_test[0:50])\n",
    "plt.plot(ps[0][0:50])\n",
    "plt.axhline(y=0.5, color='r', linestyle='-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshez[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mses[1][0:50])\n",
    "plt.axhline(y=threshez[1], color='r', linestyle='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_from_thres[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# #np.column_stack((votes[0], votes[1],votes[2],votes[3],votes[4],votes[5],vote_result,vote_result_negative,vote_result_distance,vote_result_distance_temp,y_test_list))\n",
    "np.column_stack((y_test,test,test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(distances_from_thresh[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(votes)):\n",
    "    fpsum = 0;\n",
    "    fnsum = 0;\n",
    "    performanceSum = 0;\n",
    "    for j in range(len(votes[i])):\n",
    "        if(votes[i][j] == y_test_list[j][0]):\n",
    "            performanceSum = performanceSum + 1;\n",
    "        elif((votes[i][j] == 1) & (y_test_list[j][0]==0)):\n",
    "            fnsum = fnsum + 1;\n",
    "        elif((votes[i][j] == 0) & (y_test_list[j][0]==1)):\n",
    "            fpsum = fpsum + 1;\n",
    "            \n",
    "    print(\"network \",i)\n",
    "    print(\"performance \",performanceSum/len(y_test));\n",
    "    print(\"false positive \",fpsum/len(y_test));\n",
    "    print(\"false negative \",fnsum/len(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_result = []\n",
    "vote_result_negative = []\n",
    "vote_result_distance = []\n",
    "vote_result_distance_temp = []\n",
    "vote_result_distance_temp2 = []\n",
    "vote_result_distance_p = []\n",
    "for i in range(len(y_test_list)):\n",
    "#     print(\n",
    "#     (0\n",
    "#         +votes[0][i]\n",
    "#         +votes[1][i]\n",
    "#         +votes[2][i]\n",
    "#         +votes[3][i]\n",
    "#         +votes[4][i]\n",
    "#         +votes[5][i]\n",
    "#     )/2)\n",
    "    if((0\n",
    "        +1*votes[0][i]\n",
    "        +1*votes[1][i]\n",
    "        +1*votes[2][i]\n",
    "        +1*votes[3][i]\n",
    "        +1*votes[4][i]\n",
    "        +1*votes[5][i]\n",
    "        #+votes[6][i]\n",
    "        )/6>=0.5):\n",
    "        vote_result.append(1)\n",
    "    else:\n",
    "        vote_result.append(0)\n",
    "        \n",
    "        \n",
    "      \n",
    "    if(votes_negative[0][i]==1):\n",
    "        votes_negative[0][i] = 1*votes_negative[0][i]\n",
    "    else:\n",
    "        votes_negative[0][i] = 1*votes_negative[0][i]\n",
    "     \n",
    "    if(votes_negative[1][i]==1):\n",
    "        votes_negative[1][i] = 1*votes_negative[1][i]\n",
    "    else:\n",
    "        votes_negative[1][i] = 1*votes_negative[1][i]\n",
    "        \n",
    "    if(votes_negative[2][i]==1):\n",
    "        votes_negative[2][i] = 1*votes_negative[2][i]\n",
    "    else:\n",
    "        votes_negative[2][i] = 1*votes_negative[2][i]\n",
    "        \n",
    "    if(votes_negative[3][i]==1):\n",
    "        votes_negative[3][i] = 1*votes_negative[3][i]\n",
    "    else:\n",
    "        votes_negative[3][i] = 1*votes_negative[3][i]\n",
    "        \n",
    "    if(votes_negative[4][i]==1):\n",
    "        votes_negative[4][i] = 1*votes_negative[4][i]\n",
    "    else:\n",
    "        votes_negative[4][i] = 1*votes_negative[4][i]\n",
    "        \n",
    "    if(votes_negative[5][i]==1):\n",
    "        votes_negative[5][i] = 1*votes_negative[5][i]\n",
    "    else:\n",
    "        votes_negative[5][i] = 1*votes_negative[5][i]\n",
    "\n",
    "    temp_distance2 = 0;\n",
    "    if(distances_from_thresh[0][i]>distance_from_thresh_0):\n",
    "        temp_distance2 = temp_distance2 +1;\n",
    "    else:\n",
    "        temp_distance2 = temp_distance2 -1;\n",
    "        \n",
    "    if(distances_from_thresh[1][i]>distance_from_thresh_1):\n",
    "        temp_distance2 = temp_distance2 +1;\n",
    "    else:\n",
    "        temp_distance2 = temp_distance2 -1;\n",
    "    \n",
    "    if(distances_from_thresh[2][i]>distance_from_thresh_2):\n",
    "        temp_distance2 = temp_distance2 +1;\n",
    "    else:\n",
    "        temp_distance2 = temp_distance2 -1;\n",
    "    if(distances_from_thresh[3][i]>distance_from_thresh_3):\n",
    "        temp_distance2 = temp_distance2 +1;\n",
    "    else:\n",
    "        temp_distance2 = temp_distance2 -1;\n",
    "    if(distances_from_thresh[4][i]>distance_from_thresh_4):\n",
    "        temp_distance2 = temp_distance2 +1;\n",
    "    else:\n",
    "        temp_distance2 = temp_distance2 -1;\n",
    "    if(distances_from_thresh[5][i]>distance_from_thresh_5):\n",
    "        temp_distance2 = temp_distance2 +1;\n",
    "    else:\n",
    "        temp_distance2 = temp_distance2 -1;\n",
    "        \n",
    "    temp_distance =  (distances_from_thresh[0][i]+distances_from_thresh[1][i]+distances_from_thresh[2][i]+distances_from_thresh[3][i]+distances_from_thresh[4][i]+distances_from_thresh[5][i])/6\n",
    "    temp = votes_negative[0][i]+votes_negative[1][i]+votes_negative[2][i]+votes_negative[3][i]+votes_negative[4][i]+votes_negative[5][i]\n",
    "    #+1*votes_negative[6]\n",
    "    temp2 = votes_negative[0][i],votes_negative[1][i],votes_negative[2][i],votes_negative[3][i],votes_negative[4][i],votes_negative[5][i]\n",
    "    #print(all(elem == temp2[0] for elem in temp2))\n",
    "    temp_distance_p =  (ps[0][i]+ps[1][i]+ps[2][i]+ps[3][i]+ps[4][i]+ps[5][i])/6\n",
    "    \n",
    "    if(temp>=0):\n",
    "#         if(not(all(elem > 0 for elem in temp2))):\n",
    "#             if(y_test_list[i][0]==0):\n",
    "#                 print(\"temp\",temp)\n",
    "#                 print(votes_negative[0][i])\n",
    "#                 print(votes_negative[1][i])\n",
    "#                 print(votes_negative[2][i])\n",
    "#                 print(votes_negative[3][i])\n",
    "#                 print(votes_negative[4][i])\n",
    "#                 print(votes_negative[5][i])\n",
    "#                 print(\"actual\",y_test[0][i])\n",
    "        vote_result_negative.append(1)\n",
    "    else:\n",
    "        vote_result_negative.append(0)\n",
    "#         if(not(all(elem < 0 for elem in temp2))):\n",
    "#             if(y_test_list[i][0]==1):\n",
    "#                 print(\"temp\",temp)\n",
    "#                 print(votes_negative[0][i])\n",
    "#                 print(votes_negative[1][i])\n",
    "#                 print(votes_negative[2][i])\n",
    "#                 print(votes_negative[3][i])\n",
    "#                 print(votes_negative[4][i])\n",
    "#                 print(votes_negative[5][i])\n",
    "#                 print(\"actual\",y_test[0][i])\n",
    "    vote_result_distance_temp.append(temp_distance)\n",
    "    if(temp_distance>0.5):\n",
    "        vote_result_distance.append(1)\n",
    "    else:\n",
    "        vote_result_distance.append(0)\n",
    "       \n",
    "    if(temp_distance2>=0):\n",
    "        vote_result_distance_temp2.append(1)\n",
    "    else:\n",
    "        vote_result_distance_temp2.append(0)\n",
    "    if(temp_distance_p>=0.5):\n",
    "        vote_result_distance_p.append(1)\n",
    "    else:\n",
    "        vote_result_distance_p.append(0)\n",
    "    \n",
    "fpsum = 0;\n",
    "fnsum = 0;\n",
    "performanceSum = 0;\n",
    "performanceSumNegative = 0;\n",
    "performanceSumDistance = 0;\n",
    "performanceSumDistance2 = 0;\n",
    "performanceSumDistanceP = 0;\n",
    "for j in range(len(votes[0])):\n",
    "    \n",
    "    if(vote_result[j] == y_test_list[j][0]):\n",
    "        performanceSum = performanceSum + 1;\n",
    "    elif((vote_result[j] == 1) & (y_test_list[j][0]==0)):\n",
    "        fnsum = fnsum + 1;\n",
    "    elif((vote_result[j] == 0) & (y_test_list[j][0]==1)):\n",
    "        fpsum = fpsum + 1;\n",
    "    \n",
    "    if(vote_result_negative[j] == y_test_list[j][0]):\n",
    "        performanceSumNegative = performanceSumNegative + 1;\n",
    "    if(vote_result_distance[j] == y_test_list[j][0]):\n",
    "        performanceSumDistance = performanceSumDistance + 1;\n",
    "    if(vote_result_distance_temp2[j] == y_test_list[j][0]):\n",
    "        performanceSumDistance2 = performanceSumDistance2 + 1;   \n",
    "    if(vote_result_distance_p[j] == y_test_list[j][0]):\n",
    "        performanceSumDistanceP = performanceSumDistanceP + 1;    \n",
    "        \n",
    "        \n",
    "print(\"network vote_result\")\n",
    "print(\"performance \",performanceSum/len(y_test));\n",
    "print(\"false positive \",fpsum/len(y_test));\n",
    "print(\"false negative \",fnsum/len(y_test))\n",
    "print(\"performance Negative \",performanceSumNegative/len(y_test));\n",
    "print(\"performance Distance \",performanceSumDistance/len(y_test));\n",
    "print(\"performance Distance2 \",performanceSumDistance2/len(y_test));\n",
    "print(\"performance DistanceP \",performanceSumDistanceP/len(y_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(votes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "#np.column_stack((votes[0], votes[1],votes[2],votes[3],votes[4],votes[5],vote_result,vote_result_negative,vote_result_distance,vote_result_distance_temp,y_test_list))\n",
    "np.column_stack((vote_result_distance_p,vote_result_distance_temp2,vote_result,y_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vale ta benchmark datasets :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrongPositives = 0;\n",
    "print(\"vote_result \");\n",
    "for j in range(len(vote_result)):\n",
    "    if((vote_result[j]==1) & (y_test_list[j][0]==0)):\n",
    "        wrongPositives = wrongPositives+1;\n",
    "print(wrongPositives)\n",
    "\n",
    "for i in range(len(votes)):\n",
    "    print(\"model \",i);\n",
    "    wrongPositives = 0;\n",
    "    for j in range(len(votes[i])):\n",
    "        if((votes[i][j]==1) & (y_test_list[j][0]==0)):\n",
    "            wrongPositives = wrongPositives+1;\n",
    "    print(wrongPositives)\n",
    "    \n",
    "for i in range(len(votes)):\n",
    "    print(\"model \",i);\n",
    "    wrongPositives = 0;\n",
    "    for j in range(len(votes[i])):\n",
    "        if((votes[i][j]==0) & (y_test_list[j][0]==1)):\n",
    "            wrongPositives = wrongPositives+1;\n",
    "    print(wrongPositives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# estimators = []\n",
    "\n",
    "# keras_model = KerasClassifier(build_fn=models[0], epochs=150, batch_size=256, verbose=0)    # create model\n",
    "# keras_model2 = KerasClassifier(build_fn=models[1], epochs=150, batch_size=256, verbose=0)    # create model\n",
    "# keras_model3 = KerasClassifier(build_fn=models[2], epochs=150, batch_size=256, verbose=0)    # create model\n",
    "\n",
    "# clf = [(\"DNN1\", keras_model),(\"DNN2\", keras_model2), ('DNN3', keras_model3)]\n",
    "# #ensemble = VotingClassifier(estimators)    # create the ensemble model\n",
    "\n",
    "# ensemble = VotingClassifier(estimators=clf)\n",
    "# #eclf = VotingClassifier(clf, weights=[1.0,1.0])\n",
    "# ensemble.fit(X_train,X_train)\n",
    "# y_hat = ensemble.predict(X_test)\n",
    "\n",
    "# #result = keras_model.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.optimize import minimize\n",
    "# from scipy.optimize import basinhopping\n",
    "# def getMax(array):\n",
    "#     votes_negative = [];\n",
    "#     for i in range(len(threshez)):\n",
    "#         temp = mses[i]\n",
    "#         tempvotes_negative = [];\n",
    "#         #print(\"thresh\",threshez[i])\n",
    "#         for j in range(len(y_test)):\n",
    "#             #new thresh\n",
    "#     #       if(temp[j]-threshez[i]>0):\n",
    "#     #             if(abs(temp[j]-threshez[i]>0.001)):\n",
    "#     #                 tempvotes.append(1)\n",
    "#     #                 tempvotes_negative.append(1)\n",
    "#     #             else:\n",
    "#     #                 tempvotes.append(0)\n",
    "#     #                 tempvotes_negative.append(-1)\n",
    "#             #old thresh\n",
    "#             #print(mses[i][j])\n",
    "#             if(mses[i][j]>threshez[i] ):\n",
    "#                 tempvotes_negative.append(1)\n",
    "#     #         else:\n",
    "#     #                 tempvotes.append(0)\n",
    "#             else:\n",
    "#                 tempvotes_negative.append(-1)\n",
    "#         votes_negative.append(tempvotes_negative)\n",
    "#     votes_negative = np.array(votes_negative)\n",
    "#     vote_result_negative = []\n",
    "#     for i in range(len(y_test_list)):\n",
    "\n",
    "#         if(votes_negative[0][i]==1):\n",
    "#             votes_negative[0][i] = array[0]*votes_negative[0][i]\n",
    "#         else:\n",
    "#             votes_negative[0][i] = array[1]*votes_negative[0][i]\n",
    "\n",
    "#         if(votes_negative[1][i]==1):\n",
    "#             votes_negative[1][i] = array[2]*votes_negative[1][i]\n",
    "#         else:\n",
    "#             votes_negative[1][i] = array[3]*votes_negative[1][i]\n",
    "\n",
    "#         if(votes_negative[2][i]==1):\n",
    "#             votes_negative[2][i] = array[4]*votes_negative[2][i]\n",
    "#         else:\n",
    "#             votes_negative[2][i] = array[5]*votes_negative[2][i]\n",
    "\n",
    "#         if(votes_negative[3][i]==1):\n",
    "#             votes_negative[3][i] = array[6]*votes_negative[3][i]\n",
    "#         else:\n",
    "#             votes_negative[3][i] = array[7]*votes_negative[3][i]\n",
    "\n",
    "#         if(votes_negative[4][i]==1):\n",
    "#             votes_negative[4][i] = array[8]*votes_negative[4][i]\n",
    "#         else:\n",
    "#             votes_negative[4][i] = array[9]*votes_negative[4][i]\n",
    "\n",
    "#         if(votes_negative[5][i]==1):\n",
    "#             votes_negative[5][i] = array[10]*votes_negative[5][i]\n",
    "#         else:\n",
    "#             votes_negative[5][i] = array[11]*votes_negative[5][i]\n",
    "\n",
    "\n",
    "#         temp = votes_negative[0][i]+votes_negative[1][i]+votes_negative[2][i]+votes_negative[3][i]+votes_negative[4][i]+votes_negative[5][i]\n",
    "#         #+1*votes_negative[6]\n",
    "#         temp2 = votes_negative[0][i],votes_negative[1][i],votes_negative[2][i],votes_negative[3][i],votes_negative[4][i],votes_negative[5][i]\n",
    "#         #print(all(elem == temp2[0] for elem in temp2))\n",
    "#         if(temp>=0):\n",
    "#             vote_result_negative.append(1)\n",
    "#         else:\n",
    "#             vote_result_negative.append(0)\n",
    "\n",
    "#     performanceSumNegative = 0;\n",
    "#     for j in range(len(votes[0])):\n",
    "#         if(vote_result_negative[j] == y_test_list[j][0]):\n",
    "#             performanceSumNegative = performanceSumNegative + 1;\n",
    "\n",
    "#     return -(performanceSumNegative/len(y_test))\n",
    "\n",
    "\n",
    "# # bounds = ((0, 10), (0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10),(0, 10),)\n",
    "# # res = minimize(\n",
    "\n",
    "# #     getMax,\n",
    "\n",
    "# #     x0=[1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "\n",
    "# # #     constraints=constraint,\n",
    "\n",
    "# #      bounds=bounds,\n",
    "\n",
    "# # )\n",
    "\n",
    "# res2 = basinhopping(\n",
    "#     getMax,\n",
    "#     x0=[1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "#     #niter = 100\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"first params\")\n",
    "# print(res2.x[0:2])\n",
    "# print(\"second params\")\n",
    "# print(res2.x[2:4])\n",
    "# print(\"third params\")\n",
    "# print(res2.x[4:6])\n",
    "# print(\"fourth params\")\n",
    "# print(res2.x[6:8])\n",
    "# print(\"fifth params\")\n",
    "# print(res2.x[8:10])\n",
    "# print(\"sixth params\")\n",
    "# print(res2.x[10:12])\n",
    "# print(res2)\n",
    "# print(res2.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_cardio = \"cardio.mat\"\n",
    "file_name_satellite = \"satellite.mat\"\n",
    "file_name_breastw = \"breastw.mat\"\n",
    "file_name_wine = \"wine.mat\"\n",
    "file_name_ionosphere = \"ionosphere.mat\"\n",
    "\n",
    "mat_cardio = scipy.io.loadmat(file_name_cardio)\n",
    "mat_satellite = scipy.io.loadmat(file_name_satellite)\n",
    "mat_breastw = scipy.io.loadmat(file_name_breastw)\n",
    "mat_wine = scipy.io.loadmat(file_name_wine)\n",
    "mat_ionosphere = scipy.io.loadmat(file_name_ionosphere)\n",
    "\n",
    "Xtemp = mat_cardio['X']\n",
    "ytemp = mat_cardio['y']\n",
    "X = np.array(Xtemp)\n",
    "y = np.array(ytemp)\n",
    "\n",
    "Xtemp2 = mat_satellite['X']\n",
    "ytemp2 = mat_satellite['y']\n",
    "X2 = np.array(Xtemp2)\n",
    "y2 = np.array(ytemp2)\n",
    "\n",
    "\n",
    "Xtemp3 = mat_breastw['X']\n",
    "ytemp3 = mat_breastw['y']\n",
    "X3 = np.array(Xtemp3)\n",
    "y3 = np.array(ytemp3)\n",
    "\n",
    "\n",
    "Xtemp4 = mat_wine['X']\n",
    "ytemp4 = mat_wine['y']\n",
    "X4 = np.array(Xtemp4)\n",
    "y4 = np.array(ytemp4)\n",
    "\n",
    "Xtemp5 = mat_ionosphere['X']\n",
    "ytemp5 = mat_ionosphere['y']\n",
    "X5 = np.array(Xtemp5)\n",
    "y5 = np.array(ytemp5)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "X2 = scaler.fit_transform(X2)\n",
    "X3 = scaler.fit_transform(X3)\n",
    "X4 = scaler.fit_transform(X4)\n",
    "X5 = scaler.fit_transform(X5)\n",
    "\n",
    "print(\"df_cardio\")\n",
    "df_cardio = pd.DataFrame(X);\n",
    "df_cardio.describe()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_satellite\")\n",
    "df_satellite = pd.DataFrame(X2);\n",
    "df_satellite.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_breastw\")\n",
    "df_breastw = pd.DataFrame(X3);\n",
    "df_breastw.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_wine\")\n",
    "df_wine = pd.DataFrame(X4);\n",
    "df_wine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_ionoshpere\")\n",
    "df_ionosphere = pd.DataFrame(X5);\n",
    "\n",
    "#df_ionosphere.describe()\n",
    "#df_ionosphere.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "#profile = ProfileReport(df_ionosphere)\n",
    "#profile.to_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "profile2 = ProfileReport(df_cardio)\n",
    "profile2.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile3 = ProfileReport(df_satellite)\n",
    "profile3.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile4 = ProfileReport(df_breastw)\n",
    "profile4.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile5 = ProfileReport(df_wine)\n",
    "profile5.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "\n",
    "my_report = sv.compare(df_wine,df_breastw)\n",
    "my_report.show_html() # Default arguments will generate to \"SWEETVIZ_REPORT.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
