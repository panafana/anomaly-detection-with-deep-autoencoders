{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import ipympl\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.io\n",
    "import hdf5storage\n",
    "#import matplotlib\n",
    "#matplotlib.use('nbagg')\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats.mstats import gmean \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from tensorflow.python.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, Dropout\n",
    "from tensorflow.python.keras.models import Model\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import time\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.callbacks import TensorBoard\n",
    "#indicate folder to save, plus other options\n",
    "tensorboard = TensorBoard(log_dir='./logs/run', histogram_freq=1,\n",
    "    write_graph=True, write_images=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = \"cardio.mat\"\n",
    "#file_name = \"ionosphere.mat\"\n",
    "file_name = \"satellite.mat\"\n",
    "#file_name = \"shuttle.mat\"\n",
    "#file_name = \"smtp.mat\"\n",
    "#file_name = \"wine.mat\"\n",
    "#file_name = \"arrhythmia.mat\"\n",
    "#file_name = \"thyroid.mat\"\n",
    "\n",
    "noise_amount = 2\n",
    "sparsity = 1 \n",
    "kernel_reg = 0.01\n",
    "bias_reg = 0.01\n",
    "iterations = 10\n",
    "dropout = 0.1\n",
    "firstLayerSize = 40\n",
    "midLayerSize = 15\n",
    "endLayerSize = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mat = scipy.io.loadmat('cardio.mat')\n",
    "#mat = scipy.io.loadmat('shuttle.mat')\n",
    "#\n",
    "#mat = hdf5storage.loadmat('smtp.mat')\n",
    "#mat = scipy.io.loadmat('wine.mat')\n",
    "#mat = scipy.io.loadmat('arrhythmia.mat')\n",
    "#mat = scipy.io.loadmat('thyroid.mat')\n",
    "\n",
    "mat = scipy.io.loadmat(file_name)\n",
    "#mat = scipy.io.loadmat('satellite.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "# Set callback functions to early stop training and save the best model so far\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=1)]\n",
    "#callbacks = [EarlyStopping(monitor='val_loss', patience=1),tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6435, 36)\n",
      "(6435,)\n"
     ]
    }
   ],
   "source": [
    "Xtemp = mat['X']\n",
    "ytemp = mat['y']\n",
    "X = np.array(Xtemp)\n",
    "y = np.array(ytemp)\n",
    "print(X.shape)\n",
    "y = y.reshape(X.shape[0],)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outliers in Dataset\n",
      "2036\n",
      "Contamination in Dataset\n",
      "0.3163947163947164\n",
      "(4399, 36)\n",
      "test_data.shape\n",
      "(4072, 37)\n",
      "train_data.shape\n",
      "(2363, 36)\n"
     ]
    }
   ],
   "source": [
    "outliers = 0\n",
    "clean_data = []\n",
    "contam_data = []\n",
    "\n",
    "for i in range(y.shape[0]):\n",
    "    if(y[i]==1.0):\n",
    "        outliers+=1;\n",
    "        contam_data.append(X[i])\n",
    "    else:\n",
    "        clean_data.append(X[i])\n",
    "      \n",
    "print(\"Total number of outliers in Dataset\")        \n",
    "print(outliers)\n",
    "contam = outliers/y.shape[0]\n",
    "print(\"Contamination in Dataset\")\n",
    "print(contam)\n",
    "clean_data = np.array(clean_data)\n",
    "contam_data = np.array(contam_data)\n",
    "print(clean_data.shape)\n",
    "clean_data2 = clean_data.copy()\n",
    "contam_data2 = contam_data.copy()\n",
    "# test_data_full = []\n",
    "# for i in range(176):\n",
    "#     test_data_full.append(contam_data[i])\n",
    "# one = np.ones(176)\n",
    "# test_data_full = np.array(test_data_full)\n",
    "\n",
    "# test_data_full= np.append(test_data_full,one,1)\n",
    "\n",
    "# print(test_data_full.shape)\n",
    "\n",
    "Xy = X.copy()\n",
    "Xy.shape\n",
    "Xy = np.array(Xy)\n",
    "Xy = np.insert(Xy, X.shape[1], y, axis=1)\n",
    "#print(Xy.shape)\n",
    "count = outliers\n",
    "count2 = outliers\n",
    "\n",
    "\n",
    "    \n",
    "#selecting all the outliers from dataset\n",
    "test_data =[]\n",
    "for i in range(Xy.shape[0]):\n",
    "    if(Xy[i,X.shape[1]]==1 and count>0):\n",
    "        test_data.append(Xy[i])\n",
    "        count = count-1\n",
    "    \n",
    "test_data = np.array(test_data)\n",
    "\n",
    "#selecting random inliers from train data and removing them\n",
    "indexes = np.random.choice(clean_data.shape[0], outliers, replace=False)\n",
    "\n",
    "random_clean_data = clean_data[indexes, :]\n",
    "random_clean_data = np.hstack((random_clean_data, np.zeros((random_clean_data.shape[0], 1), dtype=random_clean_data.dtype)))\n",
    "\n",
    "indexes = indexes.reshape(outliers,1)\n",
    "\n",
    "#deleting the values from train data\n",
    "clean_data = np.delete(clean_data,indexes,0)\n",
    "#print(clean_data)                \n",
    "test_data = np.append(test_data,random_clean_data,axis=0)\n",
    "np.random.shuffle(test_data)\n",
    "\n",
    "print(\"test_data.shape\")\n",
    "print(test_data.shape)\n",
    "#print(test_data.shape)\n",
    "test_x = test_data[:,:test_data.shape[1]-1]\n",
    "test_y = test_data[:,test_data.shape[1]-1]\n",
    "#print(test_x.shape)\n",
    "#print(test_y.shape)\n",
    "test_x = pd.DataFrame(test_x)\n",
    "test_y = pd.DataFrame(test_y)\n",
    "\n",
    "print(\"train_data.shape\")                    \n",
    "print(clean_data.shape)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2168,)\n",
      "Number of Outliers in Test\n",
      "2036\n",
      "Number of Inliers in Test\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = train_test_split(clean_data2,  test_size=0.03, random_state=42)\n",
    "X_test = np.append(X_test,np.zeros((X_test.shape[0],1),dtype='float64'),axis=1)\n",
    "\n",
    "inliers_test = X_test.shape[0]\n",
    "\n",
    "contam_new = contam_data2.shape[0]/X_test.shape[0]\n",
    "contam_data3 = np.append(contam_data2,np.ones((contam_data2.shape[0],1),dtype='float64'),axis=1)\n",
    "outliers_test = contam_data3.shape[0]\n",
    "X_test = np.append(X_test,contam_data3,axis=0)\n",
    "np.random.shuffle(X_test)\n",
    "Xy_test = X_test.copy\n",
    "y_test = X_test[:,X_test.shape[1]-1]\n",
    "#print(y_test.shape)\n",
    "X_test = np.delete(X_test,X_test.shape[1]-1,axis=1)\n",
    "#print(X_test.shape)\n",
    "\n",
    "X_test_np = X_test\n",
    "y_test_np = y_test\n",
    "print(y_test.shape)\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "print(\"Number of Outliers in Test\")\n",
    "print(outliers_test)\n",
    "print(\"Number of Inliers in Test\")\n",
    "print(inliers_test)\n",
    "#print(X_train)\n",
    "#print(X_train.mean())\n",
    "#print(pd.DataFrame(X_train).describe())\n",
    "X_train_noisy = X_train\n",
    "#noisy data\n",
    "#print(X_train_noisy)\n",
    "def apply_noise(col):\n",
    "    #print(col.mean())\n",
    "    mu, sigma = 0, abs(col.mean())\n",
    "    noise = np.random.normal(mu, sigma, col.shape[0]) \n",
    "    for i  in range(col.shape[0]):\n",
    "        col[i] += noise[i]\n",
    "\n",
    "    return col\n",
    "#print(X_train)\n",
    "X_train_noisy = np.apply_along_axis(apply_noise, 0, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "clean_data = scaler.fit_transform(clean_data)\n",
    "test_x  = scaler.transform(test_x)\n",
    "clean_data = pd.DataFrame(clean_data)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test  = scaler.transform(X_test)\n",
    "clean_data = pd.DataFrame(clean_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A target array with shape (4267, 36) was passed for an output of shape (None, 144) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-70c592b167c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    291\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m     )\n\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2536\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2537\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[1;32m-> 2538\u001b[1;33m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[0;32m   2539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2540\u001b[0m       \u001b[1;31m# If sample weight mode has not been set and weights are None for all the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[1;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[0;32m    741\u001b[0m           raise ValueError('A target array with shape ' + str(y.shape) +\n\u001b[0;32m    742\u001b[0m                            \u001b[1;34m' was passed for an output of shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m                            \u001b[1;34m' while using as loss `'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    744\u001b[0m                            \u001b[1;34m'This loss expects targets to have the same shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                            'as the output.')\n",
      "\u001b[1;31mValueError\u001b[0m: A target array with shape (4267, 36) was passed for an output of shape (None, 144) while using as loss `mean_squared_error`. This loss expects targets to have the same shape as the output."
     ]
    }
   ],
   "source": [
    "for k in range(iterations): \n",
    "    #deep autoencoder\n",
    "    start_time = time.time()\n",
    "\n",
    "    inputs = Input(shape=(X.shape[1],))\n",
    "    #encoded = Dense(50, activation='relu')(inputs)\n",
    "    #encoded = Dense(25, activation='relu')(inputs)\n",
    "\n",
    "    encoded = Dense(firstLayerSize, activation='relu'\n",
    "                    #,activity_regularizer=regularizers.l1(1e-3)\n",
    "                    #,kernel_regularizer=l2(kernel_reg) \n",
    "                    #,bias_regularizer=l2(bias_reg)\n",
    "                   )(inputs)\n",
    "    encoded = Dense(midLayerSize, activation='relu'\n",
    "                   # ,activity_regularizer=regularizers.l1(1e-3)\n",
    "                   #,kernel_regularizer=l2(kernel_reg) \n",
    "                   #,bias_regularizer=l2(bias_reg)\n",
    "                   )(encoded)\n",
    "\n",
    "    decoded = Dense(firstLayerSize, activation='relu'\n",
    "                   #,activity_regularizer=regularizers.l1(1e-3)\n",
    "                   #,kernel_regularizer=l2(kernel_reg) \n",
    "                   #, bias_regularizer=l2(bias_reg)\n",
    "                   )(encoded)\n",
    "    #decoded = Dense(21, activation='relu')(decoded)\n",
    "    decoded = Dense(X.shape[1], activation='tanh')(decoded)\n",
    "\n",
    "\n",
    "    autoencoder = Model(inputs, decoded)\n",
    "    for num, layer in enumerate(autoencoder.layers):\n",
    "        layer._name = 'autoencoder_' + str(num)+'_'+str(k)\n",
    "    #autoencoder.summary()\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    autoencoder.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    batch_size = 256,\n",
    "                    shuffle = False,\n",
    "                    validation_split=0.4,\n",
    "                    verbose=0\n",
    "                   )\n",
    "\n",
    "    #encoded_data = encoder.predict(test_x)\n",
    "    decoded_data = autoencoder.predict(X_test)\n",
    "    #print(dict(zip(unique, counts)))\n",
    "    #print(np.std(np.sqrt(np.power(test_x - decoded_data, 2)),axis=1))\n",
    "    mse = np.mean((np.power(X_test - decoded_data, 2)), axis=1)\n",
    "\n",
    "    mse = mse.reshape(-1, 1)\n",
    "    #print(mse.shape)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    mse = scaler.fit_transform(mse)\n",
    "    \n",
    "    \n",
    "    encoded_c = Dense(firstLayerSize, activation='relu'\n",
    "                    #,activity_regularizer=regularizers.l1(1e-3)\n",
    "                    ,kernel_regularizer=l2(kernel_reg) \n",
    "                    ,bias_regularizer=l2(bias_reg)\n",
    "                   )(inputs)\n",
    "    encoded_c = Dense(midLayerSize, activation='relu'\n",
    "                   # ,activity_regularizer=regularizers.l1(1e-3)\n",
    "                   ,kernel_regularizer=l2(kernel_reg) \n",
    "                   ,bias_regularizer=l2(bias_reg)\n",
    "                   )(encoded_c)\n",
    "\n",
    "    decoded_c = Dense(firstLayerSize, activation='relu'\n",
    "                   #,activity_regularizer=regularizers.l1(1e-3)\n",
    "                   ,kernel_regularizer=l2(kernel_reg) \n",
    "                   , bias_regularizer=l2(bias_reg)\n",
    "                   )(encoded_c)\n",
    "    #decoded = Dense(21, activation='relu')(decoded)\n",
    "    decoded_c = Dense(X.shape[1], activation='tanh')(decoded_c)\n",
    "\n",
    "\n",
    "    autoencoder_c = Model(inputs, decoded_c)\n",
    "    for num, layer in enumerate(autoencoder_c.layers):\n",
    "        layer._name = 'autoencoder_c_' + str(num)+'_'+str(k)\n",
    "    #autoencoder.summary()\n",
    "    autoencoder_c.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    autoencoder_c.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    batch_size = 256,\n",
    "                    shuffle = False,\n",
    "                    validation_split=0.4,\n",
    "                    verbose=0\n",
    "                   )\n",
    "\n",
    "    #encoded_data = encoder.predict(test_x)\n",
    "    decoded_data_c = autoencoder_c.predict(X_test)\n",
    "    #print(dict(zip(unique, counts)))\n",
    "    #print(np.std(np.sqrt(np.power(test_x - decoded_data, 2)),axis=1))\n",
    "    mse_c = np.mean((np.power(X_test - decoded_data_c, 2)), axis=1)\n",
    "\n",
    "    mse_c = mse_c.reshape(-1, 1)\n",
    "    #print(mse.shape)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    mse_c = scaler.fit_transform(mse_c)\n",
    "\n",
    "\n",
    "    #denoising autoencoder\n",
    "    encoded2_de = Dense(firstLayerSize, activation='relu'\n",
    "                     #,activity_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "                    #, kernel_regularizer=l2(kernel_reg) \n",
    "                    #, bias_regularizer=l2(bias_reg)\n",
    "                    )(inputs)\n",
    "    #encoded2 = Dense(round(X.shape[1]/6), activation='relu')(encoded2)\n",
    "    encoded2_de = Dense(midLayerSize, activation='relu'\n",
    "                    #,activity_regularizer=regularizers.l1(1e-3)\n",
    "                    )(encoded2_de)\n",
    "\n",
    "    #decoded2 = Dense(round(X.shape[1]/6), activation='relu')(encoded2)\n",
    "    decoded2_de = Dense(firstLayerSize, activation='relu'\n",
    "                   #  ,\n",
    "                    # kernel_regularizer=l2(kernel_reg), \n",
    "                   # bias_regularizer=l2(bias_reg)\n",
    "                    )(encoded2_de)\n",
    "    decoded2_de = Dense(X.shape[1], activation='tanh')(decoded2_de)\n",
    "\n",
    "    autoencoder2_de = Model(inputs, decoded2_de)\n",
    "    for num, layer in enumerate(autoencoder2_de.layers):\n",
    "        layer._name = 'autoencoder2_de_' + str(num)+'_'+str(k)\n",
    "    #autoencoder2_de.summary()\n",
    "    autoencoder2_de.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    autoencoder2_de.fit(X_train_noisy, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    shuffle=False,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_split=0.4,\n",
    "                    verbose=0\n",
    "    )\n",
    "\n",
    "    #encoded_data = encoder.predict(test_x)\n",
    "    decoded_data_de = autoencoder2_de.predict(X_test)\n",
    "\n",
    "    #print(dict(zip(unique, counts)))\n",
    "    mse_de = np.mean(np.power(X_test - decoded_data_de, 2), axis=1)\n",
    "    mse_de = mse_de.reshape(-1, 1)\n",
    "    #print(mse_de.shape)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    mse_de = scaler.fit_transform(mse_de)\n",
    "    df_error_de = pd.DataFrame({'reconstruction_error': mse_de[:,0]})\n",
    "    #df_error_de.describe()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #sparse autoencoder\n",
    "    encoded2_s = Dense(firstLayerSize, activation='relu'\n",
    "                   # ,activity_regularizer=regularizers.l1(sparsity)\n",
    "                   # , kernel_regularizer=l2(kernel_reg) \n",
    "                   # , bias_regularizer=l2(bias_reg)\n",
    "                    )(inputs)\n",
    "    #encoded2 = Dense(round(X.shape[1]/6), activation='relu')(encoded2)\n",
    "    encoded2_s = Dense(midLayerSize, activation='relu'\n",
    "                    ,activity_regularizer=regularizers.l1(sparsity)\n",
    "                    )(encoded2_s)\n",
    "\n",
    "    #decoded2 = Dense(round(X.shape[1]/6), activation='relu')(encoded2)\n",
    "    decoded2_s = Dense(firstLayerSize, activation='relu'\n",
    "                   # ,activity_regularizer=regularizers.l1(sparsity)\n",
    "                    # kernel_regularizer=l2(kernel_reg), \n",
    "                   # bias_regularizer=l2(bias_reg)\n",
    "                    )(encoded2_s)\n",
    "    decoded2_s = Dense(X.shape[1], activation='tanh')(decoded2_s)\n",
    "\n",
    "\n",
    "    autoencoder2_s = Model(inputs, decoded2_s)\n",
    "    for num, layer in enumerate(autoencoder2_s.layers):\n",
    "        layer._name = 'autoencoder2_s_' + str(num)+'_'+str(k)\n",
    "    #autoencoder2_s.summary()\n",
    "    autoencoder2_s.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    autoencoder2_s.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    shuffle=False,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_split=0.4,\n",
    "                    verbose=0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    #encoded_data = encoder.predict(test_x)\n",
    "    decoded_data_s = autoencoder2_s.predict(X_test)\n",
    "\n",
    "    #print(dict(zip(unique, counts)))\n",
    "    mse_s = np.mean(np.power(X_test - decoded_data_s, 2), axis=1)\n",
    "    mse_s = mse_s.reshape(-1, 1)\n",
    "    #print(mse_s.shape)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    mse_s = scaler.fit_transform(mse_s)\n",
    "    df_error_s = pd.DataFrame({'reconstruction_error': mse_s[:,0]})\n",
    "    #df_error_s.describe()\n",
    "     \n",
    "    #sparse autoencoder new\n",
    "    encoded3_s3 = Dense(firstLayerSize, activation='relu'\n",
    "                    #,activity_regularizer=regularizers.l1(sparsity)\n",
    "                   # ,kernel_regularizer=l2(kernel_reg)\n",
    "                   # ,bias_regularizer=l2(bias_reg)\n",
    "                     )(inputs)\n",
    "    encoded3_s3 = Dropout(dropout)(encoded3_s3)\n",
    "    decoded3_s3 = Dense(midLayerSize, activation='relu'\n",
    "                   ,activity_regularizer=regularizers.l1(sparsity)\n",
    "                   # , kernel_regularizer=l2(kernel_reg), \n",
    "                   # bias_regularizer=l2(bias_reg)\n",
    "                    )(encoded3_s3)\n",
    "    decoded3_s3 = Dropout(dropout)(decoded3_s3)\n",
    "    decoded3_s3 = Dense(firstLayerSize, activation='relu'\n",
    "                   # ,activity_regularizer=regularizers.l1(sparsity)\n",
    "                   # , kernel_regularizer=l2(kernel_reg), \n",
    "                   # bias_regularizer=l2(bias_reg)\n",
    "                    )(decoded3_s3)\n",
    "    decoded3_s3 = Dropout(dropout)(decoded3_s3)\n",
    "    decoded3_s3 = Dense(X.shape[1], activation='tanh')(decoded3_s3)\n",
    "    decoded3_s3 = Dropout(dropout)(decoded3_s3)\n",
    "\n",
    "    autoencoder3_s3 = Model(inputs, decoded3_s3)\n",
    "    \n",
    "    for num, layer in enumerate(autoencoder3_s3.layers):\n",
    "        layer._name = 'autoencoder3_s3_' + str(num)+'_'+str(k)\n",
    "        \n",
    "    autoencoder3_s3.compile(optimizer='adam', loss='mse')\n",
    "    #autoencoder3_s3.summary()\n",
    "\n",
    "    autoencoder3_s3.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_split=0.4,\n",
    "                    verbose=0\n",
    "    )\n",
    "\n",
    "    decoded_data3_s3 = autoencoder3_s3.predict(X_test)\n",
    "    mse3_s3 = np.mean(np.power(X_test - decoded_data3_s3, 2), axis=1) \n",
    "    mse3_s3 = mse3_s3.reshape(-1, 1)\n",
    "    #print(mse_s.shape)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    mse3_s3 = scaler.fit_transform(mse3_s3)\n",
    "\n",
    "    \n",
    "    ensemble = [];\n",
    "    for i in range(0,mse.size):\n",
    "            ensemble.append(0.25*mse[i][0]+0.25*mse_de[i][0]+0.25*mse_s[i]+0.25*mse3_s3[i])\n",
    "    #print(ensemble)\n",
    "\n",
    "    from tensorflow.keras.layers import concatenate\n",
    "    from tensorflow.keras.layers import Average\n",
    "    from tensorflow.keras.models import Model\n",
    "   \n",
    "    #x = Average()([decoded,decoded2_de, decoded3_s3, decoded2_s])  # merge the outputs of the two models\n",
    "    x = concatenate([decoded,decoded2_de, decoded3_s3, decoded2_s])  # merge the outputs of the two models\n",
    "\n",
    "#     out = Dense(10,activation='tanh'\n",
    "# #                 , kernel_regularizer=l2(kernel_reg) \n",
    "# #                 , bias_regularizer=l2(bias_reg)\n",
    "#                )(x)\n",
    "    out = Dense(X.shape[1],activation='tanh'\n",
    "#                 , kernel_regularizer=l2(kernel_reg) \n",
    "#                 , bias_regularizer=l2(bias_reg)\n",
    "               )(x)  # final layer of the network\n",
    "    model = Model(inputs=inputs, outputs=out)\n",
    "    for l in model.layers:\n",
    "        l.trainable = False\n",
    "\n",
    "    model.layers[len(model.layers)-1].trainable = True\n",
    "#     model.layers[len(model.layers)-2].trainable = True\n",
    "#     model.layers[len(model.layers)-3].trainable = True\n",
    "\n",
    "#     for l in model.layers:\n",
    "#         print(l.trainable)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train, X_train,\n",
    "                    epochs=500,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_split=0.4,\n",
    "                    verbose=0\n",
    "    )\n",
    "\n",
    "    ensemble_pred = model.predict(X_test)\n",
    "    #print(ensemble_pred)\n",
    "    ensemble2_mse = np.mean(np.power(X_test - ensemble_pred, 2), axis=1)\n",
    "\n",
    "\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import auc\n",
    "\n",
    "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, mse)\n",
    "    fpr_keras5, tpr_keras5, thresholds_keras5 = roc_curve(y_test, ensemble2_mse)\n",
    "    fpr_keras_de, tpr_keras_de, thresholds_keras_de = roc_curve(y_test, mse_de)\n",
    "    fpr_keras_s, tpr_keras_s, thresholds_keras_s = roc_curve(y_test, mse_s)\n",
    "    fpr_keras_s3, tpr_keras_s3, thresholds_keras_s3 = roc_curve(y_test, mse3_s3)\n",
    "    fpr_kerasE, tpr_kerasE, thresholds_kerasE = roc_curve(y_test, ensemble)\n",
    "    fpr_keras_c, tpr_keras_c, thresholds_keras_c = roc_curve(y_test, mse_c)\n",
    "    #print(thresholds_keras2)\n",
    "    auc_keras = auc(fpr_keras, tpr_keras)\n",
    "    auc_kerasE = auc(fpr_kerasE, tpr_kerasE)\n",
    "    auc_keras5 = auc(fpr_keras5, tpr_keras5)\n",
    "    auc_keras_de = auc(fpr_keras_de, tpr_keras_de)\n",
    "    auc_keras_s = auc(fpr_keras_s, tpr_keras_s)\n",
    "    auc_keras_s3 = auc(fpr_keras_s3, tpr_keras_s3)\n",
    "    auc_keras_c = auc(fpr_keras_c, tpr_keras_c)\n",
    "    #print(thresholds_keras)\n",
    "#     print('Model 1')\n",
    "#     print(auc_keras)\n",
    "#     print('Model 2')\n",
    "#     print(auc_keras2)\n",
    "#     print('Model 3')\n",
    "#     print(auc_keras3)\n",
    "#     print('Model 4')\n",
    "#     #print(auc_keras4)\n",
    "#     print('Ensemble')\n",
    "#     print(auc_kerasE)\n",
    "#     print('Ensemble2')\n",
    "#     print(auc_keras5)\n",
    "#     #print(thresholds_keras)\n",
    "#     print('Model _de')\n",
    "#     print(auc_keras_de)\n",
    "#     print('Model _s')\n",
    "#     print(auc_keras_s)\n",
    "#     print('Model _de2')\n",
    "#     print(auc_keras_de2)\n",
    "#     print('Model _s2')\n",
    "#     print(auc_keras_s2)\n",
    "    a = pd.DataFrame([auc_keras, auc_kerasE, auc_keras5, auc_keras_de, auc_keras_s, auc_keras_s3, auc_keras_c], [\"Model 1\", \"Ensemble\", \"Ensemble2\", \"Model _de\", \"Model _s\", \"Model _s3\", \"Model _c\"])\n",
    "    print(a)\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "    plt.plot(fpr_kerasE, tpr_kerasE, label='Ensemble (area = {:.3f})'.format(auc_kerasE))\n",
    "    plt.plot(fpr_keras5, tpr_keras5, label='Ensemble2 (area = {:.3f})'.format(auc_keras5))\n",
    "    plt.plot(fpr_keras_de, tpr_keras_de, label='Model _de (area = {:.3f})'.format(auc_keras_de))\n",
    "    plt.plot(fpr_keras_s, tpr_keras_s, label='Model _s (area = {:.3f})'.format(auc_keras_s))\n",
    "    plt.plot(fpr_keras_s3, tpr_keras_s3, label='Model _s3 (area = {:.3f})'.format(auc_keras_s3))\n",
    "    plt.plot(fpr_keras_c, tpr_keras_c, label='Model _c (area = {:.3f})'.format(auc_keras_c))\n",
    "    #plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    if(k==0):\n",
    "        sums = [auc_keras, auc_kerasE, auc_keras5, auc_keras_de, auc_keras_s, auc_keras_s3,auc_keras_c];\n",
    "    else:\n",
    "        temp = [auc_keras, auc_kerasE, auc_keras5, auc_keras_de, auc_keras_s, auc_keras_s3,auc_keras_c]\n",
    "        for tempi in range(0,len(sums)):\n",
    "            sums[tempi] = sums[tempi]+temp[tempi];\n",
    "\n",
    "    import csv\n",
    "    from datetime import date\n",
    "    with open('resultsAllModels1.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if(k==iterations-1):\n",
    "            today = date.today()\n",
    "            d1 = today.strftime(\"%d/%m/%Y\")\n",
    "            writer.writerow([d1])\n",
    "            writer.writerow([file_name])\n",
    "            writer.writerow([\"params\",\"noise_amount\",noise_amount,\"sparsity\" ,sparsity,\"kernel_reg\", kernel_reg,\"bias_reg\" ,bias_reg,\"iterations\" ,iterations])\n",
    "            writer.writerow([\"Model 1\", \"Ensemble\", \"Ensemble2\", \"Model _de\", \"Model _s\", \"Model _s3\", \"Model c\"])\n",
    "            writer.writerow([x/iterations for x in sums])\n",
    "    print(k)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# outputs = [layer.output for layer in autoencoder2_s.layers]\n",
    "# #print(outputs[0].eval())\n",
    "# # Initialize session \n",
    "# # or, for passing in a layer directly\n",
    "# def get_all_outputs(model, input_data, learning_phase=1):\n",
    "#     outputs = [layer.output for layer in model.layers[1:]] # exclude Input\n",
    "#     layers_fn = K.function([model.input, K.learning_phase()], outputs)\n",
    "#     return layers_fn([input_data, learning_phase])\n",
    "\n",
    "# #print(get_all_outputs(autoencoder2_s,X_test))\n",
    "# for i in range(len(get_all_outputs(autoencoder2_s,X_test)[1])):\n",
    "#     plt.plot(get_all_outputs(autoencoder2_s,X_test)[1][i])\n",
    "    \n",
    "# print((get_all_outputs(autoencoder2_s,X_test)[0][0]))\n",
    "#plt.hist(get_all_outputs(autoencoder2_s,X_test)[0][0])\n",
    "#plt.hist(get_all_outputs(autoencoder2_s,X_test)[0][1])\n",
    "#plt.hist(get_all_outputs(autoencoder2_s,X_test)[0][2])\n",
    "\n",
    "\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pd = pd.DataFrame(clean_data2)\n",
    "clean_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pd = pd.DataFrame(clean_data2)\n",
    "clean_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pd = pd.DataFrame(clean_data2)\n",
    "clean_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pd = pd.DataFrame(clean_data2)\n",
    "clean_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(get_all_outputs(autoencoder2,X_test)[1])):\n",
    "#     plt.plot(get_all_outputs(autoencoder2,X_test)[1][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keract\n",
    "from keract import get_gradients_of_trainable_weights\n",
    "from keract import get_gradients_of_activations\n",
    "\n",
    "\n",
    "a = keract.get_gradients_of_activations(autoencoder3_s3, X_train, X_train)\n",
    "#print(a)\n",
    "for key, value in a.items():\n",
    "    if('dense' in key):\n",
    "        plt.plot(a[key])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e = keract.get_gradients_of_activations(autoencoder2_s, X_train, X_train)\n",
    "#print(a)\n",
    "for key, value in e.items():\n",
    "    if('dense' in key):\n",
    "        plt.plot(e[key])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = keract.get_gradients_of_activations(autoencoder, X_train, X_train)\n",
    "#print(a)\n",
    "for key, value in w.items():\n",
    "    if('dense' in key):\n",
    "        plt.plot(w[key])\n",
    "    \n",
    "\n",
    "#plt.hist(a['dense_704/bias:0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for l in autoencoder3_s3.layers:\n",
    "        for a in l.get_weights():\n",
    "            plt.hist(a)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in autoencoder3_s2.layers:\n",
    "        for a in l.get_weights():\n",
    "            plt.hist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in autoencoder3.layers:\n",
    "        for a in l.get_weights():\n",
    "            plt.hist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in autoencoder2_de.layers:\n",
    "        for a in l.get_weights():\n",
    "            plt.hist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tempfile\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "pruning_epochs = 100\n",
    "# Load functionality for adding pruning wrappers\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute the pruning end step\n",
    "num_images = X_train.shape[0] * (1 - 0.4)\n",
    "end_step = np.ceil(num_images / 256).astype(np.int32) * pruning_epochs\n",
    "\n",
    "# Define pruning configuration\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.00,\n",
    "                                                               final_sparsity=0.9,\n",
    "                                                               begin_step=0.2*end_step,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "model_for_pruning = prune_low_magnitude(autoencoder3, **pruning_params)\n",
    "    \n",
    "\n",
    "\n",
    "# Recompile the model\n",
    "model_for_pruning.compile(optimizer='adam', loss='mse',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Model callbacks\n",
    "log_dir = '.\\logs'\n",
    "callbacks2 = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tensorflow.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch = 100000000, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None, update_freq='epoch'),\n",
    "  tfmot.sparsity.keras.PruningSummaries(\n",
    "    log_dir, update_freq='epoch'\n",
    "  )\n",
    "]\n",
    "\n",
    "# Fitting data\n",
    "model_for_pruning.fit(X_train, X_train,\n",
    "                      batch_size=256,\n",
    "                      epochs=pruning_epochs,\n",
    "                      verbose=1,\n",
    "                      callbacks=callbacks2,\n",
    "                      validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in model_for_pruning.layers:\n",
    "        for a in l.get_weights():\n",
    "            plt.hist(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_data3_p = model_for_pruning.predict(X_test)\n",
    "\n",
    "mse3_p = np.mean(np.power(X_test - decoded_data3_p, 2), axis=1)\n",
    "df_error3_p = pd.DataFrame({'reconstruction_error': mse3_p})\n",
    "fpr_kerasp, tpr_kerasp, thresholds_kerasp = roc_curve(y_test, mse3_p)\n",
    "auc_kerasp = auc(fpr_kerasp, tpr_kerasp)\n",
    "print(auc_kerasp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(len(tpr_keras)) # index for df\n",
    "roc = pd.DataFrame({'fpr' : pd.Series(fpr_keras, index=i),'tpr' : pd.Series(tpr_keras, index = i), '1-fpr' : pd.Series(1-fpr_keras, index = i), 'tf' : pd.Series(tpr_keras - (1-fpr_keras), index = i), 'thresholds' : pd.Series(thresholds_keras, index = i)})\n",
    "roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "\n",
    "# Plot tpr vs 1-fpr\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(roc['tpr'])\n",
    "plt.plot(roc['1-fpr'], color = 'red')\n",
    "plt.xlabel('1-False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "roc = pd.DataFrame({'tf' : pd.Series(tpr_keras-(1-fpr_keras), index=i), 'threshold' : pd.Series(thresholds_keras, index=i)})\n",
    "roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "#list(roc_t['threshold'])\n",
    "threshold1 = roc_t['threshold']\n",
    "print(threshold1)\n",
    "threshold1 = threshold1.to_numpy()\n",
    "threshold1 = threshold1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(len(tpr_keras2)) # index for df\n",
    "roc = pd.DataFrame({'fpr' : pd.Series(fpr_keras2, index=i),'tpr' : pd.Series(tpr_keras2, index = i), '1-fpr' : pd.Series(1-fpr_keras2, index = i), 'tf' : pd.Series(tpr_keras2 - (1-fpr_keras2), index = i), 'thresholds' : pd.Series(thresholds_keras2, index = i)})\n",
    "roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "\n",
    "# Plot tpr vs 1-fpr\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(roc['tpr'])\n",
    "plt.plot(roc['1-fpr'], color = 'red')\n",
    "plt.xlabel('1-False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "roc = pd.DataFrame({'tf' : pd.Series(tpr_keras2-(1-fpr_keras2), index=i), 'threshold' : pd.Series(thresholds_keras2, index=i)})\n",
    "roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "list(roc_t['threshold'])\n",
    "threshold2 = roc_t['threshold']\n",
    "print(threshold2)\n",
    "threshold2 = threshold2.to_numpy()\n",
    "threshold2 = threshold2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(len(tpr_keras3)) # index for df\n",
    "roc = pd.DataFrame({'fpr' : pd.Series(fpr_keras3, index=i),'tpr' : pd.Series(tpr_keras3, index = i), '1-fpr' : pd.Series(1-fpr_keras3, index = i), 'tf' : pd.Series(tpr_keras3 - (1-fpr_keras3), index = i), 'thresholds' : pd.Series(thresholds_keras3, index = i)})\n",
    "roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "\n",
    "# Plot tpr vs 1-fpr\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(roc['tpr'])\n",
    "plt.plot(roc['1-fpr'], color = 'red')\n",
    "plt.xlabel('1-False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "roc = pd.DataFrame({'tf' : pd.Series(tpr_keras3-(1-fpr_keras3), index=i), 'threshold' : pd.Series(thresholds_keras3, index=i)})\n",
    "roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "list(roc_t['threshold'])\n",
    "threshold3 = roc_t['threshold']\n",
    "print(threshold3)\n",
    "threshold3 = threshold3.to_numpy()\n",
    "threshold3 = threshold3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(len(tpr_keras4)) # index for df\n",
    "roc = pd.DataFrame({'fpr' : pd.Series(fpr_keras4, index=i),'tpr' : pd.Series(tpr_keras4, index = i), '1-fpr' : pd.Series(1-fpr_keras4, index = i), 'tf' : pd.Series(tpr_keras4 - (1-fpr_keras4), index = i), 'thresholds' : pd.Series(thresholds_keras4, index = i)})\n",
    "roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "\n",
    "# Plot tpr vs 1-fpr\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(roc['tpr'])\n",
    "plt.plot(roc['1-fpr'], color = 'red')\n",
    "plt.xlabel('1-False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "roc = pd.DataFrame({'tf' : pd.Series(tpr_keras4-(1-fpr_keras4), index=i), 'threshold' : pd.Series(thresholds_keras4, index=i)})\n",
    "roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "#list(roc_t['threshold'])\n",
    "threshold4 = roc_t['threshold']\n",
    "print(threshold4)\n",
    "threshold4 = threshold4.to_numpy()\n",
    "threshold4 = threshold4[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(auc_keras3,auc_keras2,auc_keras))\n",
    "print(min(auc_keras3,auc_keras2,auc_keras))\n",
    "print(max(auc_keras3,auc_keras2,auc_keras)-min(auc_keras3,auc_keras2,auc_keras))\n",
    "print((auc_keras3+auc_keras2+auc_keras)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble\n",
    "\n",
    "result1 = [];\n",
    "result2 = [];\n",
    "result3 = [];\n",
    "result4 = [];\n",
    "#print(threshold1)\n",
    "print('1') \n",
    "for i in range(0,mse.size):\n",
    "    if(mse[i][0]>threshold1):\n",
    "        result1.append(1);\n",
    "    else:\n",
    "        result1.append(0);\n",
    "    if(mse2[i][0]>threshold2):\n",
    "        result2.append(1);\n",
    "    else:\n",
    "        result2.append(0);\n",
    "    if(mse3[i]>threshold3):\n",
    "        result3.append(1);\n",
    "    else:\n",
    "        result3.append(0);\n",
    "    if(mse4[i]>threshold4):\n",
    "        result4.append(1);\n",
    "    else:\n",
    "        result4.append(0);\n",
    "        \n",
    "correct =0; \n",
    "correct2 =0; \n",
    "correct3 =0; \n",
    "correct4 =0; \n",
    "correctE =0;\n",
    "for i in range(0,mse.size):\n",
    "    if(result1[i] == y_test.to_numpy()[i]):\n",
    "        correct = correct + 1;\n",
    "\n",
    "print(correct/mse.size)\n",
    "print('2') \n",
    "\n",
    "for i in range(0,mse.size):\n",
    "    if(result2[i] == y_test.to_numpy()[i]):\n",
    "        correct2 = correct2 + 1;\n",
    "        \n",
    "\n",
    "print(correct2/mse.size)\n",
    "print('3') \n",
    "\n",
    "for i in range(0,mse.size):\n",
    "    if(result3[i] == y_test.to_numpy()[i]):\n",
    "        correct3 = correct3 + 1;\n",
    "print(correct3/mse.size)\n",
    "print('4')   \n",
    "\n",
    "for i in range(0,mse.size):\n",
    "    if(result4[i] == y_test.to_numpy()[i]):\n",
    "        correct4 = correct4 + 1;\n",
    "\n",
    "print(correct4/mse.size)\n",
    "print('E')  \n",
    "\n",
    "\n",
    "for i in range(0,mse.size):\n",
    "    temp = (result1[i] + result2[i] + result3[i] + result4[i])/4\n",
    "    if(temp>0.5):\n",
    "        if(1 == y_test.to_numpy()[i]):\n",
    "            correctE = correctE + 1;\n",
    "    else:\n",
    "        if(0 == y_test.to_numpy()[i]):\n",
    "            correctE = correctE + 1;\n",
    "        \n",
    "print(correctE/mse.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepstack.base import KerasMember\n",
    "\n",
    "# X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.4, random_state=23)\n",
    "\n",
    "# member1 = KerasMember(name=\"model1\", keras_model=autoencoder, train_batches=(X_train, X_train), val_batches=(X_test,X_test))\n",
    "# member2 = KerasMember(name=\"model2\", keras_model=autoencoder2, train_batches=(X_train, X_train), val_batches=(X_test,X_test))\n",
    "# member3 = KerasMember(name=\"model3\", keras_model=autoencoder3, train_batches=(X_train, X_train), val_batches=(X_test,X_test))\n",
    "# member4 = KerasMember(name=\"model4\", keras_model=autoencoder4, train_batches=(X_train, X_train), val_batches=(X_test,X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deepstack.ensemble import DirichletEnsemble\n",
    "\n",
    "# wAvgEnsemble = DirichletEnsemble()\n",
    "# wAvgEnsemble.add_members([member1, member2, member3, member4])\n",
    "# wAvgEnsemble.fit()\n",
    "# wAvgEnsemble.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Layer as layers\n",
    "# from keras.layers import  Input, Dense \n",
    "\n",
    "# from keras.models import Model\n",
    "# import keras.backend as K\n",
    "# import tensorflow as tf\n",
    "\n",
    "# class WeightedSum(layers):\n",
    "#     \"\"\"A custom keras layer to learn a weighted sum of tensors\"\"\"\n",
    "#     constraint=tf.keras.constraints.min_max_norm(max_value=1,min_value=0)\n",
    "#     def __init__(self, **kwargs):\n",
    "#         super(WeightedSum, self).__init__(**kwargs)\n",
    "\n",
    "#     def build(self, input_shape=1):\n",
    "#         self.a = self.add_weight(\n",
    "#             name='alpha',\n",
    "#             shape=(),\n",
    "#             initializer='ones',\n",
    "#             dtype='float32',\n",
    "#             trainable=True,\n",
    "#         )\n",
    "#         super(WeightedSum, self).build(input_shape)\n",
    "\n",
    "#     def call(self, model_outputs):\n",
    "#         return self.a * model_outputs[0] + (1 - self.a) * model_outputs[1]\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Weighed sum of the two models' outputs with a = 0.1\n",
    "# out = WeightedSum([autoencoder.output, autoencoder2.output])\n",
    "\n",
    "# # Create the merged model\n",
    "# model = Model(inputs=[inputs, inputs2], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
